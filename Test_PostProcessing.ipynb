{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc1688-b3a8-4c82-8d13-8c740ca92753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the 'results' folder\n",
    "results_path = 'results'\n",
    "\n",
    "# Find folders with 'REP' in the name\n",
    "folders_with_rep = [f for f in glob.glob(os.path.join(results_path, '*REP*')) if os.path.isdir(f)]\n",
    "\n",
    "# Group by base experiment name (removing the last '__number' suffix)\n",
    "grouped_exps = defaultdict(list)\n",
    "for path in folders_with_rep:\n",
    "    folder_name = os.path.basename(path)\n",
    "    if '__' in folder_name:\n",
    "        base = '__'.join(folder_name.split('__')[:-1])  # remove the last part\n",
    "        grouped_exps[base].append(path)\n",
    "    else:\n",
    "        grouped_exps[folder_name].append(path)  # fallback if no '__' present\n",
    "\n",
    "# Now you can iterate over each experiment group\n",
    "for base_name, group_paths in grouped_exps.items():\n",
    "    print(f\"\\nGroup: {base_name}:\\t{len(group_paths)} cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eef9d8-cf53-4c23-a734-3ed8b9b11a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_exps = list(grouped_exps.keys())\n",
    "\n",
    "tgt_key = key_exps[0]\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for folder in grouped_exps[tgt_key]:\n",
    "    # Extract exp_id from folder name\n",
    "    exp_id = folder.split('__')[-1]\n",
    "\n",
    "    # Define path to the parquet file\n",
    "    parquet_path = os.path.join(folder, 'Baseline_proto.parquet')\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(parquet_path):\n",
    "        # Load the data\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "\n",
    "        # Add the exp_id as a column\n",
    "        df['exp_id'] = exp_id\n",
    "\n",
    "        # Append to list\n",
    "        all_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {parquet_path} does not exist.\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Display or save the result\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b360d08-7e15-4c89-ad08-1f217a35442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "df_plot = combined_df.copy()\n",
    "\n",
    "for c in ['client_1', 'client_2']:\n",
    "    df_plot[c] = df_plot[c].str.replace('__proto', '')\n",
    "# Add pair column\n",
    "df_plot['pair'] = df_plot['client_1'] + ' vs ' + df_plot['client_2']\n",
    "\n",
    "df_plot['epoch'] = df_plot['epoch'].astype(int)\n",
    "df_plot['label'] = df_plot['label'].astype(int)\n",
    "\n",
    "epoch_slider = alt.binding_range(min=df_plot['epoch'].min(), max=df_plot['epoch'].max(), step=1, name=\"Epoch\")\n",
    "client_dropdown = alt.binding_select(options=sorted(df_plot['client_1'].unique()), name=\"Client\")\n",
    "\n",
    "epoch_select = alt.selection_point(fields=['epoch'], bind=epoch_slider, value=0)\n",
    "client_select = alt.selection_point(fields=['client_1'], bind=client_dropdown, value = 'District_A')\n",
    "\n",
    "selection = alt.selection_point(fields=['pair'], bind='legend')\n",
    "\n",
    "line = alt.Chart(df_plot).mark_line().encode(\n",
    "    x='label',\n",
    "    y='mean(cosine)',\n",
    "    color=alt.Color('pair:N', title='Client Pair'),\n",
    "    opacity=alt.when(selection).then(alt.value(1)).otherwise(alt.value(0.3))\n",
    ").add_params(\n",
    "    epoch_select, client_select, selection\n",
    ").transform_filter(\n",
    "    epoch_select & client_select\n",
    ")\n",
    "\n",
    "band = alt.Chart(df_plot).mark_errorband(extent='ci').encode(\n",
    "    x='label',\n",
    "    y=alt.Y('cosine').title('Miles/Gallon'),\n",
    "    color=alt.Color('pair:N', title='Client Pair'),\n",
    "    opacity=alt.when(selection).then(alt.value(.45)).otherwise(alt.value(0.1))\n",
    ").add_params(\n",
    "    epoch_select, client_select, selection\n",
    ").transform_filter(\n",
    "    epoch_select & client_select\n",
    ")\n",
    "\n",
    "final_plot = band + line\n",
    "\n",
    "final_plot.properties(\n",
    "        width=500,\n",
    "        height=300,\n",
    "        title=f'Experiment:{tgt_key}'\n",
    "    ).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64a12f-a3cc-4307-a12d-d4a8556a41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_cosine_with_sklearn(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Prepare an empty column for the normalized cosine\n",
    "    df['cosine_norm'] = None\n",
    "\n",
    "    # Group by (epoch, label, exp_id)\n",
    "    for (epoch, label, exp_id), group in df.groupby(['epoch', 'label', 'exp_id']):\n",
    "        scaler = MinMaxScaler()\n",
    "        # Reshape needed because scaler expects 2D array\n",
    "        normalized = scaler.fit_transform(group[['cosine']])\n",
    "        # Assign back to the correct indices\n",
    "        df.loc[group.index, 'cosine_norm'] = normalized.flatten()\n",
    "\n",
    "    return df\n",
    "\n",
    "norm_df = normalize_cosine_with_sklearn(combined_df)\n",
    "norm_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac676ac-bc80-4958-97df-2926ec29b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = norm_df.copy()\n",
    "\n",
    "for c in ['client_1', 'client_2']:\n",
    "    df_plot[c] = df_plot[c].str.replace('__proto', '')\n",
    "# Add pair column\n",
    "df_plot['pair'] = df_plot['client_1'] + ' vs ' + df_plot['client_2']\n",
    "\n",
    "df_plot['epoch'] = df_plot['epoch'].astype(int)\n",
    "df_plot['label'] = df_plot['label'].astype(int)\n",
    "\n",
    "epoch_slider = alt.binding_range(min=df_plot['epoch'].min(), max=df_plot['epoch'].max(), step=1, name=\"Epoch\")\n",
    "client_dropdown = alt.binding_select(options=sorted(df_plot['client_1'].unique()), name=\"Client\")\n",
    "\n",
    "epoch_select = alt.selection_point(fields=['epoch'], bind=epoch_slider, value=0)\n",
    "client_select = alt.selection_point(fields=['client_1'], bind=client_dropdown, value = 'District_A')\n",
    "\n",
    "selection = alt.selection_point(fields=['pair'], bind='legend')\n",
    "\n",
    "line = alt.Chart(df_plot).mark_line().encode(\n",
    "    x='label',\n",
    "    y='mean(cosine_norm)',\n",
    "    color=alt.Color('pair:N', title='Client Pair'),\n",
    "    opacity=alt.when(selection).then(alt.value(1)).otherwise(alt.value(0.3))\n",
    ").add_params(\n",
    "    epoch_select, client_select, selection\n",
    ").transform_filter(\n",
    "    epoch_select & client_select\n",
    ")\n",
    "\n",
    "band = alt.Chart(df_plot).mark_errorband(extent='ci').encode(\n",
    "    x='label',\n",
    "    y=alt.Y('cosine_norm').title('Miles/Gallon'),\n",
    "    color=alt.Color('pair:N', title='Client Pair'),\n",
    "    opacity=alt.when(selection).then(alt.value(.45)).otherwise(alt.value(0.1))\n",
    ").add_params(\n",
    "    epoch_select, client_select, selection\n",
    ").transform_filter(\n",
    "    epoch_select & client_select\n",
    ")\n",
    "\n",
    "final_plot = band + line\n",
    "\n",
    "final_plot.properties(\n",
    "        width=500,\n",
    "        height=300,\n",
    "        title=f'Experiment:{tgt_key}'\n",
    "    ).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985007c8-94da-4999-b343-4e501e6cd166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadc182-5b4f-4136-ba8d-9e4d55b2a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwe = combined_df.copy()\n",
    "qwe = qwe[qwe['epoch'] == '14']\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "for c in ['client_1', 'client_2']:\n",
    "    qwe[c] = qwe[c].str.replace('__proto', '')\n",
    "# Add pair column\n",
    "qwe['pair'] = qwe['client_1'] + ' vs ' + qwe['client_2']\n",
    "qwe = qwe[qwe['client_1'] == 'District_D']\n",
    "\n",
    "sns.lineplot(data = qwe, x = 'label', y = 'cosine', hue = 'pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ecf3ae-ff67-4992-9885-84ac58786795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47354e6d-262d-40f8-bf8e-9e25edda5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132c1b0-f6f4-4002-9bc7-067d6e89a727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c693b-307d-469e-9d24-a91d91a0510d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec019368db80ff3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:10:45.191751Z",
     "start_time": "2025-05-26T09:10:19.205814Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from datasets import Priv_NAMES as DATASET_NAMES\n",
    "from datasets import get_private_dataset\n",
    "from models import get_all_models, get_model\n",
    "from utils.Server import train, local_evaluate\n",
    "from utils.Toolbox_analysis import create_latent_df, process_latent_df\n",
    "from utils.Toolbox_postprocessing import proto_analysis, distributions_analysis\n",
    "from utils.Toolbox_visualization import load_and_scale_data, combine_latents, plot_latent_heatmap, plot_time_series_and_latents\n",
    "\n",
    "from utils.Toolbox_visualization import plot_proto_similar, plot_distribution_similar\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(description='You Only Need Me', allow_abbrev=False)\n",
    "    parser.add_argument('--device_id', type=int, default=0, help='The Device Id for Experiment')\n",
    "    parser.add_argument('--experiment_id', type=str, default='Pipeline_Full_medium_E', help='Experiment identifier')\n",
    "    parser.add_argument('--extra_coments', type=str, default='proto_month', help='Aditional info')\n",
    "    parser.add_argument('--run_simulation', type=bool, default=False, help='The Device Id for Experiment')\n",
    "    parser.add_argument('--detect_anomalies', type=bool, default=False)\n",
    "    parser.add_argument('--generate_viz', type=bool, default=True, help='Creates and saves interactive visualizations')\n",
    "\n",
    "\n",
    "    # Communication - epochs\n",
    "    parser.add_argument('--communication_epoch', type=int, default=2,\n",
    "                        help='The Communication Epoch in Federated Learning')\n",
    "    parser.add_argument('--local_epoch', type=int, default=1, help='The Local Epoch for each Participant')\n",
    "\n",
    "    # Participants info\n",
    "    parser.add_argument('--parti_num', type=int, default=None, help='The Number for Participants. If \"None\" will be setted as the sum of values described in --domain')\n",
    "    parser.add_argument('--online_ratio', type=float, default=1, help='The Ratio for Online Clients')\n",
    "    parser.add_argument('--tgt_district', type=str, default='District_E', help='Target district name.')\n",
    "    \n",
    "    # Data parameter\n",
    "    parser.add_argument('--dataset', type=str, default='fl_leaks', choices=DATASET_NAMES, help='Which scenario to perform experiments on.')\n",
    "    parser.add_argument('--domains', type=dict, default={\n",
    "                                                        'Graeme': 5,\n",
    "                                                        # 'Balerma': 3,\n",
    "                                                        },\n",
    "                        help='Domains and respective number of participants.')\n",
    "\n",
    "    ## Time series preprocessing\n",
    "    parser.add_argument('--interval_agg', type=int, default=2 * 60 ** 2,\n",
    "                        help='Agregation interval (seconds) of time series')\n",
    "    parser.add_argument('--window_size', type=int, default=84, help='Rolling window length')\n",
    "\n",
    "    # Model (AER) parameters\n",
    "    parser.add_argument('--input_size', type=int, default=5, help='Number of sensors')  #TODO adaptar\n",
    "    parser.add_argument('--output_size', type=int, default=5, help='Shape output - dense layer')\n",
    "    parser.add_argument('--lstm_units', type=int, default=30,\n",
    "                        help='Number of LSTM units (the latent space will have dimension 2 times bigger')\n",
    "    \n",
    "\n",
    "    # Federated parameters\n",
    "    parser.add_argument('--model', type=str, default='fpl', help='Federated Model name.', choices=get_all_models()) #fedavg\n",
    "\n",
    "    parser.add_argument('--structure', type=str, default='homogeneity')\n",
    "\n",
    "    parser.add_argument('--pri_aug', type=str, default='weak',  # weak strong\n",
    "                        help='Augmentation for Private Data')\n",
    "    parser.add_argument('--learning_decay', type=bool, default=False, help='The Option for Learning Rate Decay')\n",
    "    parser.add_argument('--averaging', type=str, default='weight', help='The Option for averaging strategy')\n",
    "\n",
    "    parser.add_argument('--infoNCET', type=float, default=0.02, help='The InfoNCE temperature')\n",
    "    parser.add_argument('--T', type=float, default=0.05, help='The Knowledge distillation temperature')\n",
    "    parser.add_argument('--weight', type=int, default=1, help='The Weigth for the distillation loss')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    if args.parti_num is None:\n",
    "        args.parti_num = sum(args.domains.values())\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632d6f0-d01d-4edc-b3a0-c2326f0be15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:10:45.239587Z",
     "start_time": "2025-05-26T09:10:45.225601Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.Server import local_evaluate\n",
    "from utils.Toolbox_analysis import process_latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d5239-ea95-4b7d-afd4-dc7a9f65a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "results_id = \"D_2_LL_LM_3_2_2_84_proto_NCET0.2_LSTM20\"  # replace with your actual results_id\n",
    "results_dir = f\"results/{results_id}\"\n",
    "\n",
    "# Recursively find all .parquet files\n",
    "parquet_files = glob.glob(os.path.join(results_dir, '**', '*.parquet'), recursive=True)\n",
    "\n",
    "# Print or use the list\n",
    "for file in parquet_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a773c96-eb12-40e3-b559-478302fef127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp_proto = pd.read_parquet(f'{results_dir}/Baseline_proto.parquet')\n",
    "\n",
    "int_cols = ['epoch', 'label']\n",
    "for c in int_cols:\n",
    "    df_exp_proto[c] = df_exp_proto[c].astype(int)\n",
    "    \n",
    "plot_proto_similar(df_exp_proto, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bef9f-820c-41dc-996d-f5baefeaaca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_latent = pd.read_parquet(parquet_files[1])\n",
    "# data_latent = data_latent[~data_latent['client_id'].str.contains('proto')].copy()\n",
    "\n",
    "# id_cols = ['client_id', 'label', 'epoch']\n",
    "# feat_cols = [col for col in data_latent.columns if 'x_' in col]\n",
    "# aux_agg = data_latent[id_cols + feat_cols]\n",
    "# aux_agg = aux_agg.groupby(id_cols).mean().reset_index()\n",
    "# aux_agg = aux_agg.sort_values(by=['epoch', 'label', 'client_id']).reset_index(drop=True)\n",
    "# aux_agg['client_id'] += '__proto'\n",
    "\n",
    "# int_cols = ['epoch', 'label']\n",
    "# for c in int_cols:\n",
    "#     aux_agg[c] = aux_agg[c].astype(int)\n",
    "    \n",
    "# df_exp_proto = proto_analysis(data_latent=aux_agg, normalize=True)\n",
    "\n",
    "# aux_df = df_exp_proto.copy()\n",
    "# aux_df.columns = ['epoch', 'label', 'client_2', 'client_1', 'cosine', 'manhattan', 'wavelet', 'dft', 'autocorr']\n",
    "\n",
    "# df_exp_proto = pd.concat([df_exp_proto, aux_df[df_exp_proto.columns.tolist()]])\n",
    "# df_exp_proto = df_exp_proto.sort_values(by=['epoch', 'label', 'client_1', 'client_2']).reset_index(drop=True)\n",
    "# df_exp_proto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25377494-2094-49c4-9ada-ee0ae10990b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp_latent = pd.read_parquet(f'{results_dir}/Baseline_distribution.parquet')\n",
    "\n",
    "int_cols = ['epoch', 'label']\n",
    "for c in int_cols:\n",
    "    df_exp_latent[c] = df_exp_latent[c].astype(int)\n",
    "    \n",
    "df_exp_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab780d6-072a-4a7a-8546-aabfb4d80f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_similar(df_exp_latent, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6bfe9-7b80-49ac-9faf-2c9c122093f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b745c2-37ab-44cf-b904-3272557bc0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b12f9f-25be-4ad2-a7f1-b2315e7cf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_path = 'results/D_2_LL_LM_2_1_2_84_proto_NCET0.2_LSTM20/.pkl'\n",
    "# with open(latent_path, 'rb') as f:\n",
    "#     latent_dfs = pickle.load(f)\n",
    "\n",
    "Baseline_latent_space = pd.read_parquet(parquet_files[3])\n",
    "\n",
    "qwe = Baseline_latent_space.copy()\n",
    "qwe.columns = ['client_2', 'client_1', 'cosine', 'manhattan', 'wavelet', 'dft', 'autocorr', 'id']\n",
    "\n",
    "new_qwe = pd.concat([Baseline_latent_space, qwe[Baseline_latent_space.columns.tolist()]])\n",
    "new_qwe = new_qwe.sort_values(by = ['id', 'client_1', 'client_2'])\n",
    "new_qwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cd9ad-a103-4de2-8be5-62cdeb905d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = load_and_scale_data(id_network = 'Graeme', id_experiment = 'D_2_LL_LM', tgt_district = 'District_D')\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54af559-b3eb-4bd1-881b-0a23f5774124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_latents(results_dir):\n",
    "    \"\"\"Combines all epoch latent data into a single DataFrame with metadata.\"\"\"\n",
    "\n",
    "    parquet_files = glob.glob(os.path.join(results_dir, '**', '*.parquet'), recursive=True)\n",
    "    pca_umap_files = [f for f in parquet_files if 'pca' in os.path.basename(f).lower() or 'umap' in os.path.basename(f).lower()]\n",
    "    \n",
    "\n",
    "    df_all = []\n",
    "\n",
    "    for file in pca_umap_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        df_all.append(df)\n",
    "\n",
    "    df_combined = pd.concat(df_all, ignore_index=True)\n",
    "    \n",
    "    df_combined['timestamp'] = pd.to_datetime(df_combined['timestamp'])\n",
    "    df_combined.rename(columns = {'label' : 'month', 'client_id' : 'label'}, inplace = True)\n",
    "    # df_combined['month'] = df_combined['timestamp'].dt.month\n",
    "\n",
    "    int_cols = ['epoch', 'month']\n",
    "    for c in int_cols:\n",
    "        df_combined[c] = df_combined[c].astype(int)\n",
    "\n",
    "    df_combined['hour'] = df_combined['timestamp'].dt.hour\n",
    "    df_combined['hour_filter'] = df_combined['hour'].apply(lambda x: x - 12 if x >= 12 else x)\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0533f-99b1-4895-b3ee-ebfe18888804",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_id = \"D_2_LL_LM_2_1_2_84_proto_NCET0.2_LSTM20\"  # replace with your actual results_id\n",
    "results_dir = f\"results/{results_id}\"\n",
    "\n",
    "df_combined = combine_latents(results_dir)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544abc8a-948d-4980-b6d6-404198e4273f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edeeec2-1333-4fe2-9248-0743afb6ae13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03727149-330d-44d9-859d-caaa3850a82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf85c1-67b5-44ee-ad72-c04d56070a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8e989-130e-4f88-a7b0-b2013dd0d27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b738bc4-73ab-428c-a9d6-4612d059e1f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:10:45.350707Z",
     "start_time": "2025-05-26T09:10:45.270583Z"
    }
   },
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "\n",
    "args.extra_coments = 'proto_month_DEBUGANDO'\n",
    "\n",
    "agg_int = int(args.interval_agg / 3600)\n",
    "results_id = f'{args.experiment_id}_{args.communication_epoch}_{args.local_epoch}_{agg_int}_{args.window_size}_{args.extra_coments}'\n",
    "\n",
    "logs_path = f\"results/logs_{results_id}.pkl\"\n",
    "results_path = f\"results/results_{results_id}.pkl\"\n",
    "latent_path = f\"results/latent_{results_id}.pkl\"\n",
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dda4b4da641e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:11:03.220282Z",
     "start_time": "2025-05-26T09:11:03.061285Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(results_path, 'rb') as f:\n",
    "    results_debug = pickle.load(f)\n",
    "\n",
    "with open(latent_path, 'rb') as f:\n",
    "    latent_dfs = pickle.load(f)\n",
    "\n",
    "with open(logs_path, 'rb') as f:\n",
    "    logs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95727ce7-447f-4e61-834f-d9bdb22fc966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:11:05.815613Z",
     "start_time": "2025-05-26T09:11:05.785798Z"
    }
   },
   "outputs": [],
   "source": [
    "args = logs['Baseline']['args']\n",
    "\n",
    "label_clients = [\n",
    "    'District_A', 'District_B', 'District_C', 'District_D', 'District_E',\n",
    "    'District_2A', 'District_2B', 'District_2C'\n",
    "]\n",
    "\n",
    "priv_dataset = get_private_dataset(args)\n",
    "\n",
    "backbones_list = priv_dataset.get_backbone(\n",
    "    parti_num=args.parti_num,\n",
    "    names_list=None,\n",
    "    n_series=args.input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be9933-3a19-4b01-a54c-0e741655ad49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:11:06.032855Z",
     "start_time": "2025-05-26T09:11:06.017021Z"
    }
   },
   "outputs": [],
   "source": [
    "logs['Baseline']['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ddb0f7-46fd-406b-bf14-8dbc083a6453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:11:06.699906Z",
     "start_time": "2025-05-26T09:11:06.685535Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12cddc-6a63-4381-ac8c-99a08d9862b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:31:47.539369Z",
     "start_time": "2025-05-26T09:31:13.241526Z"
    }
   },
   "outputs": [],
   "source": [
    "train_DL = priv_dataset.get_data_loaders()\n",
    "base_index = train_DL[0]['X_index']\n",
    "latent_dfs_local = {}\n",
    "\n",
    "scenarios = ['Baseline']\n",
    "\n",
    "\n",
    "for scenario in scenarios:\n",
    "    global_model_history = logs[scenario]['model']['global_weights_history']\n",
    "    for epoch in range(args.communication_epoch):\n",
    "        aux_latents = []\n",
    "        state_dict = global_model_history[epoch]\n",
    "        for net in backbones_list:\n",
    "            net.load_state_dict(state_dict)\n",
    "    \n",
    "        latent_spaces = local_evaluate(backbones_list, train_DL, priv_dataset, False, False)\n",
    "        for i, client in enumerate(latent_spaces):\n",
    "            client_lat = create_latent_df(\n",
    "                X_index=base_index,\n",
    "                x_lat=client,\n",
    "                label=f\"{label_clients[i]}__{epoch}\",\n",
    "                is_unix=True\n",
    "            )\n",
    "            aux_latents.append(client_lat)\n",
    "\n",
    "        data_latent = pd.concat(aux_latents)\n",
    "        data_latent[['client_id', 'epoch']] = data_latent['label'].str.split('__', expand = True)\n",
    "        data_latent['label'] = data_latent['timestamp'].dt.month\n",
    "    \n",
    "        id_cols = ['client_id', 'label', 'epoch']\n",
    "        feat_cols = [col for col in data_latent.columns if 'x_' in col]\n",
    "        aux_agg = data_latent[id_cols + feat_cols]\n",
    "        aux_agg = aux_agg.groupby(id_cols).mean().reset_index()\n",
    "        aux_agg['client_id'] += '__proto'\n",
    "    \n",
    "        data_latent = pd.concat([data_latent[['timestamp'] + id_cols + feat_cols], aux_agg])\n",
    "        data_latent.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        df_latent, df_pca_scaled, df_umap_scaled = process_latent_df(\n",
    "                    df_latent = data_latent,\n",
    "                    umap_neighbors=15,\n",
    "                    umap_min_dist=0.50,\n",
    "                    reduce_raw = False,\n",
    "                    id_cols =id_cols,\n",
    "                    return_scaled = False\n",
    "                )\n",
    "\n",
    "        latent_dfs[scenario][epoch] = {\n",
    "            'latent_space': df_latent,\n",
    "            'pca_scl': df_pca_scaled,\n",
    "            'umap_scl': df_umap_scaled\n",
    "        }\n",
    "\n",
    "\n",
    "# zxc = []\n",
    "# for asd in latent_dfs['Baseline'].values():\n",
    "#     qwe = asd['latent_space'].copy()\n",
    "#     qwe.drop(columns = ['hour', 'month'], inplace = True)\n",
    "#     zxc.append(qwe)\n",
    "\n",
    "# final_qwe = pd.concat(zxc)\n",
    "# data_latent.equals(final_qwe)\n",
    "\n",
    "# latent_dfs[scenario][epoch]['umap_scl'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bb146-1e52-4512-b429-3124c174818e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:32:22.212985Z",
     "start_time": "2025-05-26T09:32:22.108890Z"
    }
   },
   "outputs": [],
   "source": [
    "df_proto = latent_dfs[scenario][epoch]['latent_space'].copy()\n",
    "df_proto = df_proto[~df_proto['client_id'].str.contains('proto')]\n",
    "df_proto['client_id'] = df_proto['client_id'].str.replace('__proto', '')\n",
    "df_proto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e022a4-0aa2-48b3-94ec-291e06aea1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:36:22.843071Z",
     "start_time": "2025-05-26T09:36:22.781079Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity, rbf_kernel\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import wasserstein_distance, energy_distance\n",
    "from scipy.linalg import subspace_angles\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "def spectral_procrustes(sim_target, sim_other):\n",
    "    try:\n",
    "        _, _, disparity = procrustes(sim_target, sim_other)\n",
    "        return disparity\n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "def compute_kl_divergence(Xa, Xb, bins=30):\n",
    "    kl_scores = []\n",
    "    for i in range(Xa.shape[1]):\n",
    "        hist_a, _ = np.histogram(Xa[:, i], bins=bins, density=True)\n",
    "        hist_b, _ = np.histogram(Xb[:, i], bins=bins, density=True)\n",
    "        hist_a += 1e-8  # smooth to avoid log(0)\n",
    "        hist_b += 1e-8\n",
    "        kl = entropy(hist_a, hist_b)\n",
    "        kl_scores.append(kl)\n",
    "    return np.mean(kl_scores)\n",
    "\n",
    "def compute_js_divergence(Xa, Xb, bins=30):\n",
    "    js_scores = []\n",
    "    for i in range(Xa.shape[1]):\n",
    "        hist_a, _ = np.histogram(Xa[:, i], bins=bins, density=True)\n",
    "        hist_b, _ = np.histogram(Xb[:, i], bins=bins, density=True)\n",
    "        hist_a += 1e-8\n",
    "        hist_b += 1e-8\n",
    "        m = 0.5 * (hist_a + hist_b)\n",
    "        js = 0.5 * entropy(hist_a, m) + 0.5 * entropy(hist_b, m)\n",
    "        js_scores.append(js)\n",
    "    return np.mean(js_scores)\n",
    "\n",
    "\n",
    "# --- Drift Metrics Functions\n",
    "\n",
    "def compute_mmd(X, Y, gamma=1.0):\n",
    "    K = rbf_kernel(X, X, gamma=gamma)\n",
    "    L = rbf_kernel(Y, Y, gamma=gamma)\n",
    "    KL = rbf_kernel(X, Y, gamma=gamma)\n",
    "    return K.mean() + L.mean() - 2 * KL.mean()\n",
    "\n",
    "def subspace_alignment(X1, X2, n_components=10):\n",
    "    pca1 = PCA(n_components=n_components).fit(X1)\n",
    "    pca2 = PCA(n_components=n_components).fit(X2)\n",
    "    angles = subspace_angles(pca1.components_.T, pca2.components_.T)\n",
    "    return np.sum(np.cos(angles))\n",
    "\n",
    "def dtw_client_trajectory(df, client):\n",
    "    grouped = df[df.client_id == client].sort_values(by=\"label\")\n",
    "    monthly = grouped.groupby(\"label\")[features].mean().values\n",
    "    return monthly\n",
    "\n",
    "def run_dbscan(df_sub):\n",
    "    db = DBSCAN(eps=0.5, min_samples=5).fit(df_sub[features])\n",
    "    return db.labels_\n",
    "\n",
    "def spectral_cluster_latent(df_sub, n_components=2):\n",
    "    sim = cosine_similarity(df_sub[features])\n",
    "    embedding = SpectralEmbedding(n_components=n_components, affinity='precomputed')\n",
    "    X_trans = embedding.fit_transform(sim)\n",
    "    return X_trans\n",
    "\n",
    "def compute_mi(df, client_a, client_b, month):\n",
    "    Xa = df[(df.label == month) & (df.client_id == client_a)][features].values\n",
    "    Xb = df[(df.label == month) & (df.client_id == client_b)][features].values\n",
    "    if len(Xa) == 0 or len(Xb) == 0:\n",
    "        return np.nan\n",
    "    est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "    Xa_d = est.fit_transform(Xa)\n",
    "    Xb_d = est.fit_transform(Xb)\n",
    "    mi_scores = [mutual_info_classif(Xa_d, Xb_d[:, i], discrete_features=True).mean() for i in range(Xb_d.shape[1])]\n",
    "    return np.mean(mi_scores)\n",
    "\n",
    "def compare_clients_distribution(Xa, Xb):\n",
    "    mmd_val = compute_mmd(Xa, Xb, gamma=0.5)\n",
    "    w_dist = np.mean([wasserstein_distance(Xa[:, i], Xb[:, i]) for i in range(Xa.shape[1])])\n",
    "    e_dist = energy_distance(Xa.flatten(), Xb.flatten())\n",
    "    return mmd_val, w_dist, e_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0de99d-9403-45c5-8ce4-1270b3e5c3c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:36:23.287145Z",
     "start_time": "2025-05-26T09:36:23.261315Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Setup\n",
    "\n",
    "df = df_proto.copy()\n",
    "features = [col for col in df.columns if col.startswith(\"x_\")]\n",
    "clients = df[\"client_id\"].unique()\n",
    "months = sorted(df[\"label\"].unique())\n",
    "target_client = \"District_E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b18dd1b8e78ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:38:45.536052Z",
     "start_time": "2025-05-26T09:36:26.111014Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "for month in tqdm(months):\n",
    "    df_month = df[df.label == month]\n",
    "    X_target = df_month[df_month.client_id == target_client][features].values\n",
    "\n",
    "    try:\n",
    "        spectral_target = spectral_cluster_latent(df_month[df_month.client_id == target_client])\n",
    "    except Exception:\n",
    "        spectral_target = None\n",
    "\n",
    "    for other_client in clients:\n",
    "        if other_client == target_client:\n",
    "            continue\n",
    "\n",
    "        X_other = df_month[df_month.client_id == other_client][features].values\n",
    "        if X_target.size == 0 or X_other.size == 0:\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"month\": month,\n",
    "            \"target_client\": target_client,\n",
    "            \"other_client\": other_client,\n",
    "        }\n",
    "\n",
    "        # 1. Distribution metrics\n",
    "        start = time.time()\n",
    "        mmd_val, w_dist, e_dist = compare_clients_distribution(X_target, X_other)\n",
    "        row.update({\n",
    "            \"MMD\": mmd_val,\n",
    "            \"Wasserstein\": w_dist,\n",
    "            \"Energy\": e_dist,\n",
    "            \"Time_Dist\": time.time() - start,\n",
    "        })\n",
    "\n",
    "        # 2. Subspace alignment\n",
    "        start = time.time()\n",
    "        sa = subspace_alignment(X_target, X_other)\n",
    "        row.update({\n",
    "            \"SubspaceAlignment\": sa,\n",
    "            \"Time_Subspace\": time.time() - start,\n",
    "        })\n",
    "\n",
    "        # 3. DTW\n",
    "        start = time.time()\n",
    "        traj_target = dtw_client_trajectory(df[df.label <= month], target_client)\n",
    "        traj_other = dtw_client_trajectory(df[df.label <= month], other_client)\n",
    "        min_len = min(len(traj_target), len(traj_other))\n",
    "        dtw_val, _ = fastdtw(traj_target[:min_len], traj_other[:min_len], dist=euclidean)\n",
    "        row.update({\n",
    "            \"DTW\": dtw_val,\n",
    "            \"Time_DTW\": time.time() - start,\n",
    "        })\n",
    "\n",
    "        # 4. Mutual Information\n",
    "        start = time.time()\n",
    "        mi = compute_mi(df, target_client, other_client, month)\n",
    "        row.update({\n",
    "            \"MutualInfo\": mi,\n",
    "            \"Time_MI\": time.time() - start,\n",
    "        })\n",
    "\n",
    "        # 5. KL Divergence\n",
    "        start = time.time()\n",
    "        kl = compute_kl_divergence(X_target, X_other)\n",
    "        row.update({\n",
    "            \"KL\": kl,\n",
    "            \"Time_KL\": time.time() - start,\n",
    "        })\n",
    "\n",
    "        # 6. JS Divergence\n",
    "        start = time.time()\n",
    "        jsd = compute_js_divergence(X_target, X_other)\n",
    "        row.update({\n",
    "            \"JSD\": jsd,\n",
    "            \"Time_JSD\": time.time() - start,\n",
    "        })\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60001cca4ad69b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:38:59.350965Z",
     "start_time": "2025-05-26T09:38:59.292397Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa1247-1716-4937-8a42-25df6c168690",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for month in tqdm(months):\n",
    "    df_month = df[df.label == month]\n",
    "    X_target = df_month[df_month.client_id == target_client][features].values\n",
    "\n",
    "    try:\n",
    "        spectral_target = spectral_cluster_latent(df_month[df_month.client_id == target_client])\n",
    "    except Exception:\n",
    "        spectral_target = None  # fallback\n",
    "\n",
    "    for other_client in clients:\n",
    "        if other_client == target_client:\n",
    "            continue\n",
    "\n",
    "        X_other = df_month[df_month.client_id == other_client][features].values\n",
    "        if X_target.size == 0 or X_other.size == 0:\n",
    "            continue\n",
    "\n",
    "        # 1. Distribution metrics\n",
    "        mmd_val, w_dist, e_dist = compare_clients_distribution(X_target, X_other)\n",
    "\n",
    "        # 2. Subspace alignment\n",
    "        sa = subspace_alignment(X_target, X_other)\n",
    "\n",
    "        # 3. DTW over historical latent trajectory\n",
    "        traj_target = dtw_client_trajectory(df[df.label <= month], target_client)\n",
    "        traj_other = dtw_client_trajectory(df[df.label <= month], other_client)\n",
    "        min_len = min(len(traj_target), len(traj_other))\n",
    "        dtw_val, _ = fastdtw(traj_target[:min_len], traj_other[:min_len], dist=euclidean)\n",
    "\n",
    "        # 4. Mutual Information\n",
    "        mi = compute_mi(df, target_client, other_client, month)\n",
    "\n",
    "        # 5. KL & JSD\n",
    "        kl = compute_kl_divergence(X_target, X_other)\n",
    "        jsd = compute_js_divergence(X_target, X_other)\n",
    "\n",
    "        results.append({\n",
    "            \"month\": month,\n",
    "            \"target_client\": target_client,\n",
    "            \"other_client\": other_client,\n",
    "            \"MMD\": mmd_val,\n",
    "            \"Wasserstein\": w_dist,\n",
    "            \"Energy\": e_dist,\n",
    "            \"SubspaceAlignment\": sa,\n",
    "            \"DTW\": dtw_val,\n",
    "            \"MutualInfo\": mi,\n",
    "            \"KL\": kl,\n",
    "            \"JSD\": jsd,\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53003724-320f-4983-abe2-724cb4841c2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:56:21.243706Z",
     "start_time": "2025-05-26T09:56:21.233560Z"
    }
   },
   "outputs": [],
   "source": [
    "def drift_id(tgt_district, seed_node, income_density_mapping, drift_income, drift_density):\n",
    "    # 1. First part: first letter of tgt_district (e.g., \"District_D\" â†’ \"D\")\n",
    "    district_code = tgt_district.split('_')[-1]\n",
    "\n",
    "    # 2. Second part: Get the N-th value of income_density_mapping, where N is the index of district_code in alphabet\n",
    "    mapping_values = income_density_mapping.split('_')\n",
    "    idx = ord(district_code.upper()) - ord('A')  # A=0, B=1, ..., D=3\n",
    "    if idx < len(mapping_values):\n",
    "        density_code = mapping_values[idx]\n",
    "    else:\n",
    "        density_code = \"??\"\n",
    "\n",
    "    # 3. Third part: zip drift pairs and generate codes like \"LM\", \"LH\", etc.\n",
    "    drift_code = drift_income[0].upper() + drift_density[0].upper()\n",
    "\n",
    "    # Combine parts into final ID string\n",
    "    exp_id = f\"{district_code}_{seed_node}_{density_code}_{drift_code}\"\n",
    "    return exp_id\n",
    "\n",
    "tgt_district = 'District_D'\n",
    "income_density_mapping = 'ML_LM_LH_LL_LL'\n",
    "drift_income = ['low', 'low']\n",
    "drift_density = ['medium', 'high']\n",
    "seed_node = '2'\n",
    "for DI, DD in zip(drift_income, drift_density):\n",
    "    exp_id = drift_id(tgt_district, seed_node, income_density_mapping, DI, DD)\n",
    "    print(exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e627175-1dfc-4e65-ab05-06000a1e532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/latent_Pipeline_Full_medium_E_2_1_2_84_proto_month_DEBUGANDO.pkl', 'rb') as f:\n",
    "    latent_dfs = pickle.load(f)\n",
    "\n",
    "latent_dfs['Baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078d9e0-8720-4691-a32b-52a35c5d41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"MMD\", \"Wasserstein\", \"Energy\", \"SubspaceAlignment\", \"DTW\", \"MutualInfo\", \"KL\", \"JSD\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_results, x=\"month\", y=metric, hue=\"other_client\", marker=\"o\")\n",
    "    plt.title(f\"{metric} Drift vs {target_client}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc6029-b92a-4a30-9e87-26359b74bd80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041d33c-b6eb-4fa2-81c2-a7c0c34e45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def cosine_distance_proto(p1, p2):\n",
    "    return cosine_distances(p1.reshape(1, -1), p2.reshape(1, -1))[0, 0]\n",
    "\n",
    "\n",
    "def manhattan_distance_proto(p1, p2):\n",
    "    return np.sum(np.abs(p1 - p2))\n",
    "\n",
    "\n",
    "def wavelet_distance(protos_a, protos_b, wavelet='db1'):\n",
    "    dist = 0\n",
    "    for i in range(protos_a.shape[1]):\n",
    "        coeffs_a = pywt.dwt(protos_a[:, i], wavelet)\n",
    "        coeffs_b = pywt.dwt(protos_b[:, i], wavelet)\n",
    "        dist += np.linalg.norm(np.array(coeffs_a[0]) - np.array(coeffs_b[0]))  # Compare approx coefficients\n",
    "    return dist\n",
    "\n",
    "def dft_similarity(protos_a, protos_b):\n",
    "    fft_a = np.fft.fft(protos_a, axis=0)\n",
    "    fft_b = np.fft.fft(protos_b, axis=0)\n",
    "    return np.linalg.norm(np.abs(fft_a - fft_b))\n",
    "\n",
    "def autocorr_similarity(protos_a, protos_b, lag=1):\n",
    "    def autocorr(x, lag):\n",
    "        return np.corrcoef(x[:-lag], x[lag:])[0, 1]\n",
    "    \n",
    "    acc = 0\n",
    "    for i in range(protos_a.shape[1]):\n",
    "        a_corr = autocorr(protos_a[:, i], lag)\n",
    "        b_corr = autocorr(protos_b[:, i], lag)\n",
    "        acc += abs(a_corr - b_corr)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ef716-eede-43e8-b2e0-87321af9b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proto = latent_dfs[scenario][epoch]['latent_space'].copy()\n",
    "df_proto = df_proto[df_proto['client_id'].str.contains('proto')]\n",
    "df_proto['client_id'] = df_proto['client_id'].str.replace('__proto', '')\n",
    "df_proto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da37a01-78f6-419a-b9f8-065efafffb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df_proto.columns if col.startswith('x_')]\n",
    "clients = df_proto['client_id'].unique()\n",
    "\n",
    "# Time-aligned prototype matrix per client\n",
    "client_protos = {\n",
    "    client: df_proto[df_proto['client_id'] == client]\n",
    "        .sort_values('label')[features]\n",
    "        .values\n",
    "    for client in clients\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e95497-a6a3-4091-bc0e-77bb6e53cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "results = []\n",
    "\n",
    "for c1, c2 in combinations(clients, 2):\n",
    "    p1 = client_protos[c1]\n",
    "    p2 = client_protos[c2]\n",
    "    \n",
    "    # Ensure same shape (truncate to minimum length)\n",
    "    min_len = min(p1.shape[0], p2.shape[0])\n",
    "    p1 = p1[:min_len]\n",
    "    p2 = p2[:min_len]\n",
    "    \n",
    "    res = {\n",
    "        \"client_1\": c1,\n",
    "        \"client_2\": c2,\n",
    "        \"cosine\": cosine_distance_proto(p1.mean(axis=0), p2.mean(axis=0)),\n",
    "        \"manhattan\": manhattan_distance_proto(p1.mean(axis=0), p2.mean(axis=0)),\n",
    "        \"wavelet\": wavelet_distance(p1, p2),\n",
    "        \"dft\": dft_similarity(p1, p2),\n",
    "        \"autocorr\": autocorr_similarity(p1, p2),\n",
    "    }\n",
    "    results.append(res)\n",
    "\n",
    "df_experiment = pd.DataFrame(results)\n",
    "df_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d271fc9-1517-412d-a360-9b785ba31580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_experiment[['cosine', 'manhattan', 'wavelet', 'dft', 'autocorr']] = scaler.fit_transform(\n",
    "    df_experiment[['cosine', 'manhattan', 'wavelet', 'dft', 'autocorr']]\n",
    ")\n",
    "\n",
    "pivot = df_experiment.pivot(index='client_1', columns='client_2', values='manhattan')\n",
    "sns.heatmap(pivot, annot=True, cmap=\"viridis\")\n",
    "plt.title(\"DFT Similarity Between Clients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7eea96-1ee2-4026-8d08-cba035a8ca10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TsLeaks]",
   "language": "python",
   "name": "conda-env-TsLeaks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
