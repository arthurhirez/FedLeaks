{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf71657f62cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.Tbx_Simulation import generate_weekly_consumption_patterns, plot_node_time_series\n",
    "from utils.configs import DISTRIBUTION_PATTERNS\n",
    "\n",
    "low_config = DISTRIBUTION_PATTERNS.get('low')\n",
    "cnsp_LOW = generate_weekly_consumption_patterns(base_demand=10, n_intervals=24, n_days=7,\n",
    "                                                morning_peak=low_config[0], afternoon_peak=low_config[1],\n",
    "                                                evening_peak=low_config[2], night_consumption=low_config[3],\n",
    "                                                variation_strength=low_config[4])\n",
    "\n",
    "med_config = DISTRIBUTION_PATTERNS.get('medium')\n",
    "cnsp_MED = generate_weekly_consumption_patterns(base_demand=10, n_intervals=24, n_days=7,\n",
    "                                                morning_peak=med_config[0], afternoon_peak=med_config[1],\n",
    "                                                evening_peak=med_config[2], night_consumption=med_config[3],\n",
    "                                                variation_strength=med_config[4])\n",
    "\n",
    "high_config = DISTRIBUTION_PATTERNS.get('high')\n",
    "cnsp_HIGH = generate_weekly_consumption_patterns(base_demand=10, n_intervals=24, n_days=7,\n",
    "                                                 morning_peak=high_config[0], afternoon_peak=high_config[1],\n",
    "                                                 evening_peak=high_config[2], night_consumption=high_config[3],\n",
    "                                                 variation_strength=high_config[4])\n",
    "\n",
    "qwe = {'low': {'full_series': np.concatenate(cnsp_LOW)},\n",
    "       'med': {'full_series': np.concatenate(cnsp_MED)},\n",
    "       'high': {'full_series': np.concatenate(cnsp_HIGH)}\n",
    "       }\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for node_id in ['low', 'med', 'high']:\n",
    "    if node_id not in qwe:\n",
    "        print(f\"Node {node_id} not found.\")\n",
    "        continue\n",
    "\n",
    "    series = qwe[node_id]['full_series']\n",
    "\n",
    "    plt.plot(series, label=f'Node {node_id}')\n",
    "\n",
    "major_ticks = np.arange(0, len(series) + 1, 24)\n",
    "minor_ticks = np.arange(0, len(series) + 1, 12)\n",
    "\n",
    "plt.xticks(major_ticks)\n",
    "plt.xticks(minor_ticks, minor=True)\n",
    "plt.grid(which='minor', alpha=0.2)\n",
    "plt.grid(which='major', alpha=0.75)\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Consumption')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.02, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba026f1a85eab3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:43:20.342156Z",
     "start_time": "2025-05-20T16:43:20.163691Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "folders = [\n",
    "    f.replace('my_collection\\\\', '') for f in glob.glob(f'my_collection/Balerma*')\n",
    "    if os.path.isdir(f)\n",
    "]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92eddb0ff7303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:44:42.321353Z",
     "start_time": "2025-05-20T16:44:42.270025Z"
    }
   },
   "outputs": [],
   "source": [
    "args.id_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e108f7d395df522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:45:03.155379Z",
     "start_time": "2025-05-20T16:45:01.082793Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from multiprocessing.managers import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wntr\n",
    "from atmn import ScenarioCollection\n",
    "\n",
    "for case in folders:\n",
    "    # Load generated scenarios using the ScenarioCollection class\n",
    "    my_collection = ScenarioCollection('my_collection')\n",
    "    my_scenario = my_collection.get_scenario(case)\n",
    "\n",
    "    save_path = f'../data_leaks/{args.id_network}/{case.replace(\"Balerma_\", \"\").replace(\"_Random_Multiple\", \"\")}'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for feature in ['Flow', 'Demand', 'Pressure']:\n",
    "        leak_configs = my_scenario.list_configs()['LeakConfigs']\n",
    "\n",
    "        aux_data = []  # Melted data for plotting\n",
    "        aux_data_leak = []  # Raw leak time series\n",
    "\n",
    "        var_id = 'pipeId' if feature.lower() == 'flow' else 'node'\n",
    "\n",
    "        # Process each leak scenario for this feature\n",
    "        for leak_scenario in leak_configs:\n",
    "            data_scenario = my_scenario.get(leak_scenario, 'DefaultSensors', 'GT')\n",
    "            df_feat = data_scenario[feature.lower()].copy()\n",
    "\n",
    "            # Ensure clean numeric formatting\n",
    "            for c in df_feat.columns:\n",
    "                df_feat[c] = df_feat[c].astype(float).round(6)\n",
    "\n",
    "            aux_leak = df_feat.copy()\n",
    "            df_feat['time'] = pd.to_datetime(df_feat.index, unit='s')\n",
    "\n",
    "            # Reshape for long-format plotting or time-series analysis\n",
    "            df_feat_melted = df_feat.melt(\n",
    "                ignore_index=False,\n",
    "                var_name=var_id,\n",
    "                value_name=feature\n",
    "            ).reset_index()\n",
    "\n",
    "            df_feat_melted = df_feat_melted.rename(columns={'time': 'Time'})\n",
    "            df_feat_melted['scenario'] = leak_scenario\n",
    "\n",
    "            aux_data.append(df_feat_melted)\n",
    "            aux_leak['scenario'] = leak_scenario\n",
    "            aux_data_leak.append(aux_leak)\n",
    "\n",
    "        # Save full leak time series and melted DataFrame\n",
    "        df_leak = pd.concat(aux_data_leak).reset_index()\n",
    "        df_leak = df_leak.rename(columns={'time': 'timestamp'})\n",
    "\n",
    "        df_leak.to_csv(f'{save_path}/{feature}.csv',\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2847a-b62d-482d-a8d2-158ca1c532c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:44:19.535663Z",
     "start_time": "2025-05-20T16:44:19.507098Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from utils.Tbx_Pipeline import load_assign_network, run_scenarios, compile_results\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(description=\"Run water network simulation with specified parameters.\")\n",
    "\n",
    "    # Identification\n",
    "    parser.add_argument('--id_network', type=str, default='Graeme', help='Identifier for the water network model.')\n",
    "    # parser.add_argument('--id_network', type=str, default='Balerma', help='Identifier for the water network model.')\n",
    "    parser.add_argument('--id_exp', type=str, default='DEBUG_PIPELINE_PATTERNS', help='Experiment identifier.')\n",
    "\n",
    "    # Features drift\n",
    "    parser.add_argument('--tgt_district', type=str, default='District_A', help='Target district name.')\n",
    "    parser.add_argument('--seed_node', type=str, default='106', help='Seed node for simulation.')  # 415\n",
    "    parser.add_argument('--income_density_mapping', type=dict,\n",
    "                        default=\n",
    "                        [('low', 'low'),\n",
    "                         ('low', 'low'),\n",
    "                         ('low', 'high'),\n",
    "                         ('medium', 'medium'),\n",
    "                         ('high', 'low')],\n",
    "                        # [('low', 'medium'),\n",
    "                        #  ('low', 'low'),\n",
    "                        #  ('low', 'low')\n",
    "                        #  ],\n",
    "                        help='List of income:density category mappings. Format: income:density')\n",
    "\n",
    "    # Time variables\n",
    "    parser.add_argument('--n_segments', type=int, default=7, help='Number of urban growth simulation segments.')\n",
    "    parser.add_argument('--epochs_lenght', type=int, default=6, help='Number of epochs per simulation run.')\n",
    "    parser.add_argument('--days_lenght', type=int, default=5, help='Number of days in each epoch.')\n",
    "    parser.add_argument('--n_intervals', type=int, default=24, help='Number of time intervals per day.')\n",
    "\n",
    "    # Experiment value\n",
    "    parser.add_argument('--value_exp', type=float, default=0.15, help='Leak severity multiplier.')\n",
    "\n",
    "    # Hydraulic variables\n",
    "    parser.add_argument('--unit', type=str, default='l/s', help='Measurement unit for flow.')\n",
    "    parser.add_argument('--morning_peak', type=float, default=1.0, help='Multiplier for morning consumption peak.')\n",
    "    parser.add_argument('--afternoon_peak', type=float, default=0.75, help='Multiplier for afternoon consumption peak.')\n",
    "    parser.add_argument('--evening_peak', type=float, default=1.25, help='Multiplier for evening consumption peak.')\n",
    "    parser.add_argument('--night_consumption', type=float, default=0.25, help='Multiplier for night-time consumption.')\n",
    "\n",
    "    return parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b74bcb16f0192e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T09:59:52.065043Z",
     "start_time": "2025-05-20T09:59:48.725322Z"
    }
   },
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "\n",
    "# Set up environment\n",
    "scripts_path = r\"C:\\Users\\arthu\\anaconda3\\envs\\TsLeaks\\Scripts\"\n",
    "os.environ[\"PATH\"] = scripts_path + os.pathsep + os.environ[\"PATH\"]\n",
    "\n",
    "# Load network and assign buildings\n",
    "wn, consumption_patterns, data_consumption = load_assign_network(\n",
    "    directory='networks\\\\original\\\\',\n",
    "    save_assignments=True,\n",
    "    verbose=True,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cdb7f4951cd7d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T09:59:54.435400Z",
     "start_time": "2025-05-20T09:59:54.419829Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "#\n",
    "# with open(f'networks/assignments/Graeme_DEBUG_PIPELINE_District_A_final.pkl', 'rb') as file:\n",
    "#     district_nodes = pickle.load(file)\n",
    "#\n",
    "# tgt_nodes = list(district_nodes['District_A']['assignments'].growth_nodes[-1])\n",
    "# data_buildings = district_nodes['District_A']['assignments'].data_buildings\n",
    "#\n",
    "# from utils.Tbx_Simulation import concatenate_consumption_patterns, plot_node_time_series\n",
    "#\n",
    "# CP = concatenate_consumption_patterns(consumption_patterns)\n",
    "# plot_node_time_series(time_series_dict=CP, node_ids=tgt_nodes, title_prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b40741c866fb90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:00:35.903461Z",
     "start_time": "2025-05-20T09:59:54.803281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run scenarios\n",
    "auto_leaks = run_scenarios(\n",
    "    water_network=wn,\n",
    "    consumption_patterns=consumption_patterns,\n",
    "    data_consumption=data_consumption,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b64e05fac45b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:01:33.638454Z",
     "start_time": "2025-05-20T10:01:31.343999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile and save results\n",
    "compile_results(\n",
    "    leaks_scenarios=auto_leaks,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424758d9c3184d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:01:36.317673Z",
     "start_time": "2025-05-20T10:01:36.257914Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'networks/assignments/Graeme_DEBUG_PIPELINE_District_A_initial.pkl', 'rb') as file:\n",
    "    district_nodes = pickle.load(file)\n",
    "\n",
    "data_buildings = district_nodes['District_A']['assignments'].data_buildings\n",
    "data_buildings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3227c09da5e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:03:37.556918Z",
     "start_time": "2025-05-20T10:03:37.485491Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'networks/assignments/Graeme_DEBUG_PIPELINE_PATTERNS_District_A_final.pkl', 'rb') as file:\n",
    "    district_nodes = pickle.load(file)\n",
    "\n",
    "data_buildings = district_nodes['District_A']['assignments'].data_buildings\n",
    "data_buildings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0b32f52837423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:05:03.246597Z",
     "start_time": "2025-05-20T10:05:02.674312Z"
    }
   },
   "outputs": [],
   "source": [
    "import wntr\n",
    "\n",
    "wn = wntr.network.WaterNetworkModel(f'networks/{args.id_network}_{args.id_exp}.inp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495a41f-34f6-46a6-b114-fc3f08449b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d51de-9b73-4267-bda3-a730155d0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from utils.Tbx_Simulation import convert_flow_rate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ---- Prepare full DataFrame (as you already have) ----\n",
    "records = []\n",
    "for district_name, district_data in district_nodes.items():\n",
    "    data_buildings = district_data['assignments'].data_buildings\n",
    "    for node in data_buildings['node_id']:\n",
    "        junction = wn.get_node(node)\n",
    "        base_demand = convert_flow_rate(junction.base_demand, 'l/s')['m3/month']\n",
    "        pattern = wn.get_pattern(junction.demand_timeseries_list[0].pattern_name).multipliers\n",
    "        consumption = base_demand * pattern\n",
    "        for t, value in enumerate(consumption):\n",
    "            records.append({\n",
    "                'district': district_name,\n",
    "                'node': node,\n",
    "                'time': t,\n",
    "                'base_demand': base_demand,\n",
    "                'pattern_value': pattern[t],\n",
    "                'consumption': value\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df['day'] = df['time'] // 24\n",
    "df['hour'] = df['time'] % 24\n",
    "\n",
    "# ---- Shared district selection ----\n",
    "district_selection = alt.selection_point(\n",
    "    fields=['district'],\n",
    "    bind=alt.binding_select(options=sorted(df['district'].unique()), name=\"Select District: \"),\n",
    "    value=sorted(df['district'].unique())[0]\n",
    ")\n",
    "\n",
    "# ---- Node selection for highlight ----\n",
    "node_selection = alt.selection_point(\n",
    "    fields=['node'],\n",
    "    # bind='legend'\n",
    "    # clear='mouseout',  # Deselect when mouse leaves\n",
    "    # empty='none'  # Don't highlight anything by default\n",
    ")\n",
    "\n",
    "# ---- Daily stacked area + line plot (with node selection) ----\n",
    "aux_feats = []\n",
    "for district_name, district_data in district_nodes.items():\n",
    "    data_buildings = district_data['assignments'].data_buildings\n",
    "    aux_feats.append(data_buildings[['node_id', 'density']])\n",
    "\n",
    "node_feats = pd.concat(aux_feats)\n",
    "node_feats.rename(columns = {'node_id' : 'node'}, inplace = True)\n",
    "\n",
    "df_agg = df.groupby(['district', 'day', 'node'], as_index=False).agg(consumption=('consumption', 'sum'))\n",
    "df_agg = pd.merge(df_agg, node_feats, how = 'left', on = 'node')\n",
    "df_total = df.groupby(['district', 'day'], as_index=False).agg(total=('consumption', 'sum'))\n",
    "\n",
    "\n",
    "area_chart = alt.Chart(df_agg).mark_area(opacity=0.5).encode(\n",
    "    x=alt.X('day:Q', title='Day'),\n",
    "    y=alt.Y('consumption:Q', stack='zero', title='Daily Consumption'),\n",
    "    color=alt.Color('node:N', scale=alt.Scale(scheme='category20b')),\n",
    "    tooltip=['node', 'day', 'consumption', 'density'],\n",
    ").add_params(\n",
    "    district_selection,\n",
    "    node_selection\n",
    ").transform_filter(\n",
    "    district_selection\n",
    ").encode(\n",
    "    opacity=alt.condition(node_selection, alt.value(1), alt.value(0.25))\n",
    ")\n",
    "\n",
    "line_chart = alt.Chart(df_total).mark_line(color='black', strokeWidth=5).encode(\n",
    "    x='day:Q',\n",
    "    y='total:Q',\n",
    "    tooltip=['day', 'total']\n",
    ").transform_filter(\n",
    "    district_selection\n",
    ")\n",
    "\n",
    "daily_plot = (area_chart + line_chart).properties(\n",
    "    width=400,\n",
    "    height=400,\n",
    "    title=\"Daily Consumption Patterns by Node and District\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Ridge plot (highlight selected node) ----\n",
    "def get_top_nodes(df, top_n=30):\n",
    "    top_nodes_df = (\n",
    "        df.groupby(['district', 'node'])['consumption'].sum()\n",
    "        .reset_index()\n",
    "        .sort_values(['district', 'consumption'], ascending=[True, False])\n",
    "    )\n",
    "    top_nodes_df['rank'] = top_nodes_df.groupby('district')['consumption'].rank(method='first', ascending=False)\n",
    "    return top_nodes_df[top_nodes_df['rank'] <= top_n][['district', 'node']]\n",
    "\n",
    "\n",
    "top_nodes_df = get_top_nodes(df)\n",
    "df_filtered = df.merge(top_nodes_df, on=['district', 'node'])\n",
    "\n",
    "hourly_mean = df_filtered.groupby(['district', 'node', 'hour'], as_index=False)['consumption'].mean()\n",
    "\n",
    "\n",
    "def minmax_scale(group):\n",
    "    scaler = MinMaxScaler(feature_range=(0.5, 1.0))\n",
    "    # Reshape to 2D array for sklearn compatibility\n",
    "    scaled_values = scaler.fit_transform(group[['consumption']])\n",
    "    group['consumption_scaled'] = scaled_values\n",
    "    return group\n",
    "\n",
    "\n",
    "hourly_scaled = hourly_mean.groupby(['district', 'node']).apply(minmax_scale, include_groups=False).reset_index()\n",
    "\n",
    "step = 20\n",
    "overlap = 1\n",
    "\n",
    "ridge_chart = alt.Chart(hourly_scaled, height=step).mark_area(\n",
    "    interpolate='monotone',\n",
    "    fillOpacity=0.75,\n",
    "    stroke='lightgray',\n",
    "    strokeWidth=0.25\n",
    ").encode(\n",
    "    x=alt.X('hour:Q', title='Hour of Day', scale=alt.Scale(domain=[0, 23])),\n",
    "    y=alt.Y('consumption_scaled:Q', axis=None, scale=alt.Scale(range=[step, -step * overlap])),\n",
    "    fill=alt.condition(\n",
    "        node_selection,\n",
    "        alt.Color('node:N', scale=alt.Scale(scheme='category20b'), legend=None),\n",
    "        alt.value('gray')\n",
    "    )\n",
    ").transform_filter(\n",
    "    district_selection\n",
    ").facet(\n",
    "    row=alt.Row('node:N').title(None).header(labelAngle=0, labelAlign='left'),\n",
    "    spacing=5,\n",
    ").add_params(\n",
    "    node_selection\n",
    ").properties(\n",
    "    title='Scaled Hourly Consumption Profile per Node by District',\n",
    "    bounds='flush'\n",
    ").resolve_scale(\n",
    "    y='independent'  # optional, helps if some nodes have different ranges\n",
    ")\n",
    "\n",
    "# ---- Combine both ----\n",
    "final_chart = daily_plot | ridge_chart\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa076ef-238c-4e34-bfe7-f46c1b772504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ab19d-ef20-4376-bca0-c1913bfda2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from utils.Tbx_Simulation import convert_flow_rate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ---- Prepare full DataFrame (as you already have) ----\n",
    "records = []\n",
    "for district_name, district_data in district_nodes.items():\n",
    "    data_buildings = district_data['assignments'].data_buildings\n",
    "    for node in data_buildings['node_id']:\n",
    "        junction = wn.get_node(node)\n",
    "        base_demand = convert_flow_rate(junction.base_demand, 'l/s')['m3/month']\n",
    "        pattern = wn.get_pattern(junction.demand_timeseries_list[0].pattern_name).multipliers\n",
    "        consumption = base_demand * pattern\n",
    "        for t, value in enumerate(consumption):\n",
    "            records.append({\n",
    "                'district': district_name,\n",
    "                'node': node,\n",
    "                'time': t,\n",
    "                'base_demand': base_demand,\n",
    "                'pattern_value': pattern[t],\n",
    "                'consumption': value\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df['day'] = df['time'] // 24\n",
    "df['hour'] = df['time'] % 24\n",
    "\n",
    "# ---- Shared district selection ----\n",
    "district_selection = alt.selection_point(\n",
    "    fields=['district'],\n",
    "    bind=alt.binding_select(options=sorted(df['district'].unique()), name=\"Select District: \"),\n",
    "    value=sorted(df['district'].unique())[0]\n",
    ")\n",
    "\n",
    "# ---- Node selection for highlight ----\n",
    "node_selection = alt.selection_point(\n",
    "    fields=['node'],\n",
    "    # bind='legend'\n",
    "    # clear='mouseout',  # Deselect when mouse leaves\n",
    "    # empty='none'  # Don't highlight anything by default\n",
    ")\n",
    "\n",
    "date_range = (0,29)\n",
    "day_interval = alt.selection_interval(encodings=['x'], value={'x': date_range}, name=\"Select Day Range\")\n",
    "\n",
    "\n",
    "# ---- Daily stacked area + line plot (with node selection) ----\n",
    "aux_feats = []\n",
    "for district_name, district_data in district_nodes.items():\n",
    "    data_buildings = district_data['assignments'].data_buildings\n",
    "    aux_feats.append(data_buildings[['node_id', 'density']])\n",
    "\n",
    "node_feats = pd.concat(aux_feats)\n",
    "node_feats.rename(columns = {'node_id' : 'node'}, inplace = True)\n",
    "\n",
    "df_agg = df.groupby(['district', 'day', 'node'], as_index=False).agg(consumption=('consumption', 'sum'))\n",
    "df_agg = pd.merge(df_agg, node_feats, how = 'left', on = 'node')\n",
    "df_total = df.groupby(['district', 'day'], as_index=False).agg(total=('consumption', 'sum'))\n",
    "\n",
    "\n",
    "# Add the day_interval selection param\n",
    "background = alt.Chart(df_agg).mark_area(opacity=0.1).encode(\n",
    "    x=alt.X('day:Q', title='Day'),\n",
    "    y=alt.Y('consumption:Q', stack='zero', title='Daily Consumption'),\n",
    "    color=alt.value('lightgray'),  # background always light gray\n",
    "    tooltip=['node', 'day', 'consumption', 'density'],\n",
    "    opacity=alt.condition(node_selection, alt.value(1), alt.value(0.2))\n",
    ").add_params(\n",
    "    district_selection,\n",
    "    node_selection,\n",
    "    day_interval\n",
    ").transform_filter(\n",
    "    district_selection\n",
    ")\n",
    "\n",
    "# Selected area overlay (only the filtered day range)\n",
    "selected = alt.Chart(df_agg).mark_area(opacity=0.6).encode(\n",
    "    x='day:Q',\n",
    "    y=alt.Y('consumption:Q', stack='zero'),\n",
    "    color=alt.Color('node:N', scale=alt.Scale(scheme='category20b')),\n",
    "    tooltip=['node', 'day', 'consumption', 'density'],\n",
    "    opacity=alt.condition(node_selection, alt.value(1), alt.value(0.4))\n",
    ").transform_filter(\n",
    "    district_selection\n",
    ").transform_filter(\n",
    "    day_interval\n",
    ")\n",
    "\n",
    "# Combine both layers\n",
    "consumption_area = background + selected\n",
    "\n",
    "# Light gray background line (all days)\n",
    "line_background = alt.Chart(df_total).mark_line(\n",
    "    color='lightgray',\n",
    "    strokeWidth=2\n",
    ").encode(\n",
    "    x='day:Q',\n",
    "    y='total:Q',\n",
    "    tooltip=['day', 'total']\n",
    ").transform_filter(\n",
    "    district_selection\n",
    ")\n",
    "\n",
    "# Black overlay line (only selected days)\n",
    "line_selected = alt.Chart(df_total).mark_line(\n",
    "    color='black',\n",
    "    strokeWidth=4\n",
    ").encode(\n",
    "    x='day:Q',\n",
    "    y='total:Q',\n",
    "    tooltip=['day', 'total']\n",
    ").transform_filter(\n",
    "    district_selection\n",
    ").transform_filter(\n",
    "    day_interval\n",
    ")\n",
    "\n",
    "# Combine both lines\n",
    "consumption_line = line_background + line_selected\n",
    "\n",
    "\n",
    "daily_plot = (consumption_area + consumption_line).properties(\n",
    "    width=400,\n",
    "    height=300,\n",
    "    title=\"Daily Consumption Patterns by Node and District\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Ridge plot (highlight selected node) ----\n",
    "def get_top_nodes(df, top_n=30):\n",
    "    top_nodes_df = (\n",
    "        df.groupby(['district', 'node'])['consumption'].sum()\n",
    "        .reset_index()\n",
    "        .sort_values(['district', 'consumption'], ascending=[True, False])\n",
    "    )\n",
    "    top_nodes_df['rank'] = top_nodes_df.groupby('district')['consumption'].rank(method='first', ascending=False)\n",
    "    return top_nodes_df[top_nodes_df['rank'] <= top_n][['district', 'node']]\n",
    "\n",
    "\n",
    "top_nodes_df = get_top_nodes(df)\n",
    "df_filtered = df.merge(top_nodes_df, on=['district', 'node'])\n",
    "df_filtered = pd.merge(df_filtered, node_feats, how = 'left', on = 'node')\n",
    "hourly_mean = df_filtered.groupby(['district', 'node', 'hour'], as_index=False)['consumption'].mean()\n",
    "\n",
    "\n",
    "def minmax_scale(group):\n",
    "    scaler = MinMaxScaler(feature_range=(0.5, 1.0))\n",
    "    # Reshape to 2D array for sklearn compatibility\n",
    "    scaled_values = scaler.fit_transform(group[['consumption']])\n",
    "    group['consumption_scaled'] = scaled_values\n",
    "    return group\n",
    "\n",
    "\n",
    "hourly_scaled = hourly_mean.groupby(['district', 'node']).apply(minmax_scale, include_groups=False).reset_index()\n",
    "\n",
    "step = 15\n",
    "overlap = .5\n",
    "\n",
    "ridge_chart = alt.Chart(df_filtered, height=step).transform_filter(\n",
    "    district_selection\n",
    ").transform_filter(\n",
    "    day_interval\n",
    ").transform_aggregate(\n",
    "    mean_consumption='mean(consumption)',\n",
    "    groupby=['node', 'density', 'hour']\n",
    ").transform_window(\n",
    "    min_val='min(mean_consumption)',\n",
    "    max_val='max(mean_consumption)',\n",
    "    groupby=['node']\n",
    ").transform_calculate(\n",
    "    consumption_scaled=\"0.5 + 0.5 * (datum.mean_consumption - datum.min_val) / (datum.max_val - datum.min_val + 1e-8)\"\n",
    ").mark_area(\n",
    "    interpolate='monotone',\n",
    "    fillOpacity=0.75,\n",
    "    stroke='lightgray',\n",
    "    strokeWidth=0.25\n",
    ").encode(\n",
    "    x=alt.X('hour:Q', title='Hour of Day', scale=alt.Scale(domain=[0, 23])),\n",
    "    y=alt.Y('consumption_scaled:Q', axis=None, scale=alt.Scale(range=[step, -step * overlap])),\n",
    "    fill=alt.condition(\n",
    "        node_selection,\n",
    "        alt.Color('node:N', scale=alt.Scale(scheme='category20b'), legend=None),\n",
    "        alt.value('gray')\n",
    "    ),\n",
    "    tooltip=['node', 'density'],\n",
    ").facet(\n",
    "    row=alt.Row('node:N').title(None).header(labelAngle=0, labelAlign='left'),\n",
    "    spacing=5,\n",
    ").add_params(\n",
    "    node_selection\n",
    ").properties(\n",
    "    title='Scaled Hourly Consumption Profile per Node by District',\n",
    "    bounds='flush'\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "# ---- Combine both ----\n",
    "final_chart = daily_plot | ridge_chart\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de57245-33a8-440f-89ec-b103c2b0e713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TsLeaks]",
   "language": "python",
   "name": "conda-env-TsLeaks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
