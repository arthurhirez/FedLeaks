{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95ec26-1198-449e-ab3e-e54d455ead2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from datasets import Priv_NAMES as DATASET_NAMES\n",
    "from datasets import get_private_dataset\n",
    "from models import get_all_models, get_model\n",
    "from utils.Server import train\n",
    "from utils.Toolbox_analysis import create_latent_df, process_latent_df\n",
    "from utils.Toolbox_visualization import format_latent_dict, load_and_scale_data, combine_latents, plot_latent_heatmap, plot_time_series_and_latents\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(description='You Only Need Me', allow_abbrev=False)\n",
    "    parser.add_argument('--device_id', type=int, default=0, help='The Device Id for Experiment')\n",
    "    parser.add_argument('--run_simulation', type=bool, default=True, help='The Device Id for Experiment')\n",
    "    parser.add_argument('--detect_anomalies', type=bool, default=False)\n",
    "    parser.add_argument('--generate_viz', type=bool, default=True, help='Creates and saves interactive visualizations')\n",
    "\n",
    "\n",
    "    # Communication - epochs\n",
    "    parser.add_argument('--communication_epoch', type=int, default=15,\n",
    "                        help='The Communication Epoch in Federated Learning')\n",
    "    parser.add_argument('--local_epoch', type=int, default=1, help='The Local Epoch for each Participant')\n",
    "\n",
    "    # Participants info\n",
    "    parser.add_argument('--parti_num', type=int, default=None, help='The Number for Participants. If \"None\" will be setted as the sum of values described in --domain')\n",
    "    parser.add_argument('--online_ratio', type=float, default=1, help='The Ratio for Online Clients')\n",
    "\n",
    "    # Data parameter\n",
    "    parser.add_argument('--dataset', type=str, default='fl_leaks', choices=DATASET_NAMES, help='Which scenario to perform experiments on.')\n",
    "    parser.add_argument('--experiment_id', type=str, default='Pipeline_Full_medium_E', help='Experiment identifier')\n",
    "    parser.add_argument('--extra_coments', type=str, default='proto_month_0.2_20', help='Aditional info')\n",
    "    parser.add_argument('--domains', type=dict, default={\n",
    "                                                        'Graeme': 5,\n",
    "                                                        # 'Balerma': 3,\n",
    "                                                        },\n",
    "                        help='Domains and respective number of participants.')\n",
    "\n",
    "    ## Time series preprocessing\n",
    "    parser.add_argument('--interval_agg', type=int, default=2 * 60 ** 2,\n",
    "                        help='Agregation interval (seconds) of time series')\n",
    "    parser.add_argument('--window_size', type=int, default=84, help='Rolling window length')\n",
    "\n",
    "    # Model (AER) parameters\n",
    "    parser.add_argument('--input_size', type=int, default=5, help='Number of sensors')  #TODO adaptar\n",
    "    parser.add_argument('--output_size', type=int, default=5, help='Shape output - dense layer')\n",
    "    parser.add_argument('--lstm_units', type=int, default=30,\n",
    "                        help='Number of LSTM units (the latent space will have dimension 2 times bigger')\n",
    "    \n",
    "\n",
    "    # Federated parameters\n",
    "    parser.add_argument('--model', type=str, default='fpl', help='Federated Model name.', choices=get_all_models()) #fedavg\n",
    "\n",
    "    parser.add_argument('--structure', type=str, default='homogeneity')\n",
    "\n",
    "    parser.add_argument('--pri_aug', type=str, default='weak',  # weak strong\n",
    "                        help='Augmentation for Private Data')\n",
    "    parser.add_argument('--learning_decay', type=bool, default=False, help='The Option for Learning Rate Decay')\n",
    "    parser.add_argument('--averaging', type=str, default='weight', help='The Option for averaging strategy')\n",
    "\n",
    "    parser.add_argument('--infoNCET', type=float, default=0.02, help='The InfoNCE temperature')\n",
    "    parser.add_argument('--T', type=float, default=0.05, help='The Knowledge distillation temperature')\n",
    "    parser.add_argument('--weight', type=int, default=1, help='The Weigth for the distillation loss')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    if args.parti_num is None:\n",
    "        args.parti_num = sum(args.domains.values())\n",
    "\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bb7c0-dcc7-474b-8a95-4059c75b3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_int = 2\n",
    "results_id = f'{args.experiment_id}_{args.communication_epoch}_{args.local_epoch}_{agg_int}_{args.window_size}_{args.extra_coments}'\n",
    "\n",
    "results_path = f\"results/results_{results_id}.pkl\"\n",
    "latent_path = f\"results/latent_{results_id}.pkl\"\n",
    "\n",
    "\n",
    "with open(results_path, 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668549ff-a898-4fdb-a5df-f90a0656ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Baseline']['model'].local_history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3baf9-a718-4bd6-a8ba-c70d6c8970b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['Baseline']['model'].global_history[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe291e1-a97a-4c58-8c8a-536c3b57c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def flatten_prototypes(local_history):\n",
    "    \"\"\"Flatten local_history into a DataFrame with prototype features as columns.\"\"\"\n",
    "    records = []\n",
    "\n",
    "    for epoch, clients in local_history.items():\n",
    "        for client_id, client_data in enumerate(clients):\n",
    "            for label, prototype in client_data.items():\n",
    "                prototype = np.array(prototype)\n",
    "                record = {\n",
    "                    'epoch': epoch,\n",
    "                    'client_id': client_id,\n",
    "                    'label': label,\n",
    "                }\n",
    "                for i, val in enumerate(prototype):\n",
    "                    record[f'feature_{i}'] = val\n",
    "                records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_by_epoch(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Normalize prototype features using MinMaxScaler separately for each epoch.\n",
    "    \n",
    "    Returns:\n",
    "    - df_normalized: DataFrame with normalized features\n",
    "    - scalers_by_epoch: dict of epoch -> fitted MinMaxScaler\n",
    "    \"\"\"\n",
    "    scalers_by_epoch = {}\n",
    "    df_list = []\n",
    "\n",
    "    for epoch, group in df.groupby(\"epoch\"):\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_features = scaler.fit_transform(group[feature_cols])\n",
    "        scaled_df = group.copy()\n",
    "        scaled_df[feature_cols] = scaled_features\n",
    "        df_list.append(scaled_df)\n",
    "        scalers_by_epoch[epoch] = scaler\n",
    "\n",
    "    df_normalized = pd.concat(df_list, ignore_index=True)\n",
    "    return df_normalized, scalers_by_epoch\n",
    "\n",
    "\n",
    "def reduce_dims(X, method=None, n_components=2, umap_neighbors=50, umap_min_dist=0.95):\n",
    "    \"\"\"\n",
    "    Applies PCA and UMAP to the input data.\n",
    "\n",
    "    Returns:\n",
    "    - X_pca: PCA-reduced data\n",
    "    - X_umap: UMAP-reduced data\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    if method == 'PCA':\n",
    "        return X_pca, None\n",
    "\n",
    "    reducer = umap.UMAP(n_components=n_components, n_neighbors=umap_neighbors, min_dist=umap_min_dist)\n",
    "    X_umap = reducer.fit_transform(X)\n",
    "\n",
    "    if method == 'UMAP':\n",
    "        return None, X_umap\n",
    "\n",
    "    return X_pca, X_umap\n",
    "\n",
    "\n",
    "def process_prototypes(local_history, method=None):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    - Flatten\n",
    "    - Normalize (MinMax) per epoch\n",
    "    - Dimensionality reduction\n",
    "\n",
    "    Returns:\n",
    "    - df_final: DataFrame with reduced dimensions\n",
    "    - scalers_by_epoch: dict of epoch -> MinMaxScaler\n",
    "    \"\"\"\n",
    "    df = flatten_prototypes(local_history)\n",
    "    feature_cols = [col for col in df.columns if col.startswith(\"feature_\")]\n",
    "\n",
    "    # MinMax scale per epoch\n",
    "    df_normalized, scalers_by_epoch = normalize_by_epoch(df, feature_cols)\n",
    "\n",
    "    # Dimensionality reduction\n",
    "    X = df_normalized[feature_cols].values\n",
    "    X_pca, X_umap = reduce_dims(X, method=method)\n",
    "\n",
    "    # Append to DataFrame\n",
    "    if X_pca is not None:\n",
    "        df_normalized[['pca_0', 'pca_1']] = X_pca\n",
    "    if X_umap is not None:\n",
    "        df_normalized[['umap_0', 'umap_1']] = X_umap\n",
    "\n",
    "    return df_normalized, scalers_by_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b0115-22ac-4576-8fdc-94d4134334d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_history = results['Baseline']['model'].local_history\n",
    "df_final, scalers_by_epoch = process_prototypes(local_history, method=None)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5802db-d93e-46a5-8aef-1093273a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def plot_prototypes(df, method='umap'):\n",
    "    \"\"\"\n",
    "    Plots reduced prototype data using Altair with interactive filters for epoch and label.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame from process_prototypes\n",
    "    - method: 'umap' or 'pca'\n",
    "    \"\"\"\n",
    "    assert method in ['umap', 'pca'], \"method must be 'umap' or 'pca'\"\n",
    "    \n",
    "    x_col = f'{method}_0'\n",
    "    y_col = f'{method}_1'\n",
    "\n",
    "    # Epoch dropdown selector\n",
    "    epoch_selector = alt.binding_select(options=sorted(df['epoch'].unique()), name=\"Epoch\")\n",
    "    epoch_selection = alt.selection_point(\n",
    "        fields=['epoch'],\n",
    "        bind=epoch_selector\n",
    "    )\n",
    "\n",
    "    # Label dropdown selector\n",
    "    label_selector = alt.binding_select(options=sorted(df['label'].unique()), name=\"Label\")\n",
    "    label_selection = alt.selection_point(\n",
    "        fields=['label'],\n",
    "        bind=label_selector\n",
    "    )\n",
    "\n",
    "    # Base chart\n",
    "    chart = alt.Chart(df).mark_point(filled=True, size=100).encode(\n",
    "        x=alt.X(x_col, title=f\"{method.upper()} 1\"),\n",
    "        y=alt.Y(y_col, title=f\"{method.upper()} 2\"),\n",
    "        color=alt.Color('label:N', title='Label'),\n",
    "        shape=alt.Shape('client_id:N', title='Client ID'),\n",
    "        tooltip=['epoch', 'client_id', 'label']\n",
    "    ).add_params(\n",
    "        epoch_selection,\n",
    "        label_selection\n",
    "    ).transform_filter(\n",
    "        epoch_selection\n",
    "    ).transform_filter(\n",
    "        label_selection\n",
    "    ).properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        title=f'{method.upper()} Projection of Prototypes (Interactive)'\n",
    "    ).interactive()  # enables zoom and pan\n",
    "\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b7226-4bb6-4d94-9e92-a583427f728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def compute_distance_lines(df, x_col, y_col):\n",
    "    \"\"\"\n",
    "    Compute pairwise distances between points,\n",
    "    grouped by epoch and label (filtered view).\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for (epoch, label), group in df.groupby(['epoch', 'label']):\n",
    "        coords = group[[x_col, y_col]].values\n",
    "        indices = group.index.values\n",
    "\n",
    "        for (i, j) in combinations(range(len(coords)), 2):\n",
    "            x1, y1 = coords[i]\n",
    "            x2, y2 = coords[j]\n",
    "            dist = np.linalg.norm(coords[i] - coords[j])\n",
    "            lines.append({\n",
    "                'epoch': epoch,\n",
    "                'label': label,\n",
    "                'x1': x1, 'y1': y1,\n",
    "                'x2': x2, 'y2': y2,\n",
    "                'distance': dist\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(lines)\n",
    "    \n",
    "def plot_prototypes_with_distances(df, method='umap'):\n",
    "    \"\"\"\n",
    "    Interactive Altair plot with:\n",
    "    - Epoch slider\n",
    "    - Label dropdown\n",
    "    - Distance lines only within selected epoch and label\n",
    "    \"\"\"\n",
    "    assert method in ['umap', 'pca'], \"method must be 'umap' or 'pca'\"\n",
    "    x_col = f'{method}_0'\n",
    "    y_col = f'{method}_1'\n",
    "\n",
    "    # Precompute distances grouped by epoch + label\n",
    "    df_lines = compute_distance_lines(df, x_col, y_col)\n",
    "\n",
    "    # Interactive widgets\n",
    "    epoch_slider = alt.binding_range(min=int(df['epoch'].min()),\n",
    "                                     max=int(df['epoch'].max()),\n",
    "                                     step=1, name=\"Epoch\")\n",
    "    epoch_selection = alt.selection_point(fields=['epoch'], bind=epoch_slider)\n",
    "\n",
    "    label_selector = alt.binding_range(min=int(df['label'].min()),\n",
    "                                     max=int(df['label'].max()),\n",
    "                                     step=1, name=\"Label\")\n",
    "    label_selection = alt.selection_point(fields=['label'], bind=label_selector)\n",
    "\n",
    "    # Distance lines\n",
    "    line_chart = alt.Chart(df_lines).mark_line(opacity=0.2).encode(\n",
    "        x='x1:Q', y='y1:Q',\n",
    "        x2='x2:Q', y2='y2:Q',\n",
    "        strokeWidth=alt.StrokeWidth('distance:Q', \n",
    "            # scale=alt.Scale(domain=[0, df_lines['distance'].max()], range=[0.5, 5])\n",
    "                                   ),\n",
    "        tooltip=['distance']\n",
    "    ).transform_filter(\n",
    "        epoch_selection\n",
    "    ).transform_filter(\n",
    "        label_selection\n",
    "    )\n",
    "\n",
    "    # Prototype scatter points\n",
    "    point_chart = alt.Chart(df).mark_point(filled=True, size=100).encode(\n",
    "        x=alt.X(x_col, title=f\"{method.upper()} 1\"),\n",
    "        y=alt.Y(y_col, title=f\"{method.upper()} 2\"),\n",
    "        color=alt.Color('label:N', title='Label'),\n",
    "        shape=alt.Shape('client_id:N', title='Client ID'),\n",
    "        tooltip=['epoch', 'client_id', 'label']\n",
    "    ).transform_filter(\n",
    "        epoch_selection\n",
    "    ).transform_filter(\n",
    "        label_selection\n",
    "    )\n",
    "\n",
    "    # Combine\n",
    "    chart = (line_chart + point_chart).add_params(\n",
    "        epoch_selection,\n",
    "        label_selection\n",
    "    ).properties(\n",
    "        width=300,\n",
    "        height=300,\n",
    "        title=f'{method.upper()} Projection with Pairwise Distances (Filtered)'\n",
    "    ).interactive()\n",
    "\n",
    "    return chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ba0d1-824c-445a-8397-0374a65dfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_path = f\"results/latent_Pipeline_Full_medium_E_15_1_2_84_proto_month.pkl\"\n",
    "\n",
    "with open(latent_path, 'rb') as f:\n",
    "    latent = pickle.load(f)\n",
    "\n",
    "latent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceebcd5-f357-4ed8-a9f1-dc114903e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "aux_proto = []\n",
    "aux_latent = []\n",
    "\n",
    "for epoch in list(latent['Baseline'].keys()):\n",
    "    data = latent['Baseline'][epoch]['latent_space'].copy()\n",
    "    data['month'] = data['timestamp'].dt.month\n",
    "    \n",
    "    data_agg = data.drop(columns=['timestamp', 'week', 'hour'])\n",
    "    data_agg = data_agg.groupby(['label', 'month']).mean().reset_index()\n",
    "\n",
    "    feature_cols = [col for col in data.columns if col.startswith('x_')]\n",
    "    \n",
    "    x_data = data[feature_cols].values\n",
    "    x_proto = data_agg[feature_cols].values\n",
    "    \n",
    "    X_raw = np.concatenate([x_data, x_proto])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X_raw)\n",
    "    \n",
    "    X_pca_scaled, X_umap_scaled = reduce_dims(\n",
    "        X=X_scaled,\n",
    "        method=None,\n",
    "        n_components=2,\n",
    "        umap_neighbors=50,\n",
    "        umap_min_dist=0.95\n",
    "    )\n",
    "\n",
    "    red_proto = X_pca_scaled[-60:, :]\n",
    "    \n",
    "    df_proto = data_agg[['month', 'label']].copy()\n",
    "    df_proto['pca_0'], df_proto['pca_1'] = red_proto[:,0], red_proto[:,1]\n",
    "    \n",
    "    red_proto = X_umap_scaled[-60:, :]\n",
    "    df_proto['umap_0'], df_proto['umap_1'] = red_proto[:,0], red_proto[:,1]\n",
    "    \n",
    "    df_proto['epoch'] = epoch\n",
    "    df_proto.rename(columns = {'label' : 'client_id', 'month' : 'label'}, inplace = True)\n",
    "\n",
    "    aux_proto.append(df_proto)\n",
    "\n",
    "\n",
    "    red_latent = X_pca_scaled[:-60, :]\n",
    "    df_latent = data[['month', 'label']].copy()\n",
    "    df_latent['pca_0'], df_latent['pca_1'] = red_latent[:,0], red_latent[:,1]\n",
    "    \n",
    "    red_latent = X_umap_scaled[:-60, :]\n",
    "    df_latent['umap_0'], df_latent['umap_1'] = red_latent[:,0], red_latent[:,1]\n",
    "    \n",
    "    df_latent['epoch'] = epoch\n",
    "    df_latent.rename(columns = {'label' : 'client_id', 'month' : 'label'}, inplace = True)\n",
    "\n",
    "    aux_latent.append(df_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15efa390-8e26-44cb-9305-7a52e24fcbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_distance_lines_with_clients(df, x_col, y_col):\n",
    "    \"\"\"\n",
    "    Compute pairwise distances between clients,\n",
    "    grouped by epoch and label. Keeps track of client IDs.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for (epoch, label), group in df.groupby(['epoch', 'label']):\n",
    "        coords = group[[x_col, y_col]].values\n",
    "        client_ids = group['client_id'].values\n",
    "\n",
    "        for (i, j) in combinations(range(len(coords)), 2):\n",
    "            x1, y1 = coords[i]\n",
    "            x2, y2 = coords[j]\n",
    "            dist = np.linalg.norm(coords[i] - coords[j])\n",
    "            lines.append({\n",
    "                'epoch': epoch,\n",
    "                'label': label,\n",
    "                'client_id': client_ids[i],\n",
    "                'client2': client_ids[j],\n",
    "                'distance': dist,\n",
    "                'x1': x1, 'y1': y1,\n",
    "                'x2': x2, 'y2': y2,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(lines)\n",
    "\n",
    "\n",
    "\n",
    "def plot_prototypes_with_distances(df, method='umap'):\n",
    "    \"\"\"\n",
    "    Interactive Altair plot with:\n",
    "    - Epoch slider\n",
    "    - Label dropdown\n",
    "    - Distance lines only within selected epoch and label\n",
    "    \"\"\"\n",
    "    assert method in ['umap', 'pca'], \"method must be 'umap' or 'pca'\"\n",
    "    x_col = f'{method}_0'\n",
    "    y_col = f'{method}_1'\n",
    "\n",
    "    # Precompute distances grouped by epoch + label\n",
    "    df_lines = compute_distance_lines_with_clients(df, x_col, y_col)\n",
    "    aux = [df_lines]\n",
    "    for client in df_plot['client_id'].unique().tolist():\n",
    "        df_lines_aux = df_lines[df_lines['client2'] == client].copy()\n",
    "        df_lines_aux.rename(columns = {'client_id' : 'client2', 'client2' : 'client_id'}, inplace = True)\n",
    "        df_lines_aux = df_lines_aux[df_lines.columns]\n",
    "        aux.append(df_lines_aux)\n",
    "    df_lines = pd.concat(aux)\n",
    "\n",
    "    # Interactive widgets\n",
    "    epoch_slider = alt.binding_range(min=int(df['epoch'].min()),\n",
    "                                     max=int(df['epoch'].max()),\n",
    "                                     step=1, name=\"Epoch\")\n",
    "    epoch_selection = alt.selection_point(fields=['epoch'], bind=epoch_slider, value = df['epoch'].min())\n",
    "\n",
    "    label_options = [None] + sorted(df['label'].unique().tolist())\n",
    "    label_selection = alt.selection_point(fields=['label'], bind=alt.binding_select(options=label_options, name = 'Labels: '), value = df['label'].min())\n",
    "\n",
    "    # Distance lines\n",
    "    line_chart = alt.Chart(df_lines).mark_line(opacity=0.2).encode(\n",
    "        x='x1:Q', y='y1:Q',\n",
    "        x2='x2:Q', y2='y2:Q',\n",
    "        strokeWidth=alt.StrokeWidth('distance:Q', \n",
    "            # scale=alt.Scale(domain=[0, df_lines['distance'].max()], range=[0.5, 5])\n",
    "                                   ),\n",
    "        tooltip=['distance']\n",
    "    ).transform_filter(\n",
    "        epoch_selection\n",
    "    ).transform_filter(\n",
    "        label_selection\n",
    "    )\n",
    "\n",
    "    # Prototype scatter points\n",
    "    point_chart = alt.Chart(df).mark_point(filled=True, size=100).encode(\n",
    "        x=alt.X(x_col, title=f\"{method.upper()} 1\"),\n",
    "        y=alt.Y(y_col, title=f\"{method.upper()} 2\"),\n",
    "        color=alt.Color('label:N', title='Label'),\n",
    "        shape=alt.Shape('client_id:N', title='Client ID'),\n",
    "        tooltip=['epoch', 'client_id', 'label']\n",
    "    ).transform_filter(\n",
    "        epoch_selection\n",
    "    ).transform_filter(\n",
    "        label_selection\n",
    "    )\n",
    "\n",
    "    # Combine\n",
    "    chart = (line_chart + point_chart).add_params(\n",
    "        epoch_selection,\n",
    "        label_selection\n",
    "    ).properties(\n",
    "        width=300,\n",
    "        height=300,\n",
    "        title=f'{method.upper()} Projection with Pairwise Distances (Filtered)'\n",
    "    ).interactive()\n",
    "\n",
    "        \n",
    "            \n",
    "    # New client selector\n",
    "    client_selector = alt.binding_select(options=sorted(df['client_id'].unique()), name=\"Client\")\n",
    "    client_selection = alt.selection_point(fields=['client_id'], bind=client_selector, value = 'District_A')\n",
    "        \n",
    "    # Line plot over labels\n",
    "    line_summary_chart = alt.Chart(df_lines).mark_line(point=True).encode(\n",
    "        x=alt.X('label:O', title='Label'),\n",
    "        y=alt.Y('distance:Q', title='Distance to Selected Client'),\n",
    "        color=alt.Color('client2:N', title='Other Client', legend=None),\n",
    "        tooltip=['label', 'distance', 'client2']\n",
    "    ).add_params(\n",
    "        client_selection, epoch_selection\n",
    "    ).transform_filter(\n",
    "        client_selection & epoch_selection\n",
    "    ).properties(\n",
    "        width=300,\n",
    "        height=200,\n",
    "        title='Distance to Selected Client across Labels (Selected Epoch)'\n",
    "    )\n",
    "    \n",
    "    return chart | line_summary_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c05de-9a3c-4c58-a490-d0db3d340778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.concat(aux_proto)\n",
    "df_plot.reset_index(drop = True, inplace = True)\n",
    "df_plot['client_id'] = df_plot['client_id'].str.split('__').str[1]\n",
    "\n",
    "plot = plot_prototypes_with_distances(df_plot, method='pca')\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c23bab-f0fe-42dd-98a8-79e7acf2d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot.save('results/imgs/proto_base_50_0.2_20.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e33f13-7f1d-4277-b0f6-8ef522bf8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latent = pd.concat(aux_latent)\n",
    "df_latent.reset_index(drop = True, inplace = True)\n",
    "df_latent['client_id'] = df_latent['client_id'].str.split('__').str[1]\n",
    "\n",
    "df_latent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80888612-3093-4be6-9bf8-25bf56d694c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwe = df_latent[(df_latent['client_id'] == 'District_A') & (df_latent['epoch'] == 14)]\n",
    "qwe[qwe['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee719de-eb10-4efe-a090-c80a56bf6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = latent['Baseline'][14]['latent_space'].copy()\n",
    "data['month'] = data['timestamp'].dt.month\n",
    "\n",
    "feature_cols = [col for col in data.columns if col.startswith('x_')]\n",
    "\n",
    "x_data = data[feature_cols].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_scl = scaler.fit_transform(x_data)\n",
    "\n",
    "df_scl = data[['month', 'label']].copy()\n",
    "\n",
    "for i, feat in enumerate(feature_cols):\n",
    "    df_scl[feat] = x_scl[:, i]\n",
    "    \n",
    "df_scl.rename(columns = {'label' : 'client_id', 'month' : 'label'}, inplace = True)\n",
    "df_scl['client_id'] = df_scl['client_id'].str.split('__').str[1]\n",
    "\n",
    "df_scl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e374649-ee28-4e3c-8bb1-92b48940771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity, rbf_kernel\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import wasserstein_distance, energy_distance\n",
    "from scipy.linalg import subspace_angles\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- Drift Metrics Functions\n",
    "\n",
    "def compute_mmd(X, Y, gamma=1.0):\n",
    "    K = rbf_kernel(X, X, gamma=gamma)\n",
    "    L = rbf_kernel(Y, Y, gamma=gamma)\n",
    "    KL = rbf_kernel(X, Y, gamma=gamma)\n",
    "    return K.mean() + L.mean() - 2 * KL.mean()\n",
    "\n",
    "def subspace_alignment(X1, X2, n_components=10):\n",
    "    pca1 = PCA(n_components=n_components).fit(X1)\n",
    "    pca2 = PCA(n_components=n_components).fit(X2)\n",
    "    angles = subspace_angles(pca1.components_.T, pca2.components_.T)\n",
    "    return np.sum(np.cos(angles))\n",
    "\n",
    "def dtw_client_trajectory(df, client):\n",
    "    grouped = df[df.client_id == client].sort_values(by=\"label\")\n",
    "    monthly = grouped.groupby(\"label\")[features].mean().values\n",
    "    return monthly\n",
    "\n",
    "def run_dbscan(df_sub):\n",
    "    db = DBSCAN(eps=0.5, min_samples=5).fit(df_sub[features])\n",
    "    return db.labels_\n",
    "\n",
    "def spectral_cluster_latent(df_sub, n_components=2):\n",
    "    sim = cosine_similarity(df_sub[features])\n",
    "    embedding = SpectralEmbedding(n_components=n_components, affinity='precomputed')\n",
    "    X_trans = embedding.fit_transform(sim)\n",
    "    return X_trans\n",
    "\n",
    "def compute_mi(df, client_a, client_b, month):\n",
    "    Xa = df[(df.label == month) & (df.client_id == client_a)][features].values\n",
    "    Xb = df[(df.label == month) & (df.client_id == client_b)][features].values\n",
    "    if len(Xa) == 0 or len(Xb) == 0:\n",
    "        return np.nan\n",
    "    est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "    Xa_d = est.fit_transform(Xa)\n",
    "    Xb_d = est.fit_transform(Xb)\n",
    "    mi_scores = [mutual_info_classif(Xa_d, Xb_d[:, i], discrete_features=True).mean() for i in range(Xb_d.shape[1])]\n",
    "    return np.mean(mi_scores)\n",
    "\n",
    "def compare_clients_distribution(Xa, Xb):\n",
    "    mmd_val = compute_mmd(Xa, Xb, gamma=0.5)\n",
    "    w_dist = np.mean([wasserstein_distance(Xa[:, i], Xb[:, i]) for i in range(Xa.shape[1])])\n",
    "    e_dist = energy_distance(Xa.flatten(), Xb.flatten())\n",
    "    return mmd_val, w_dist, e_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a61245-075b-498d-a73f-13d8c2053512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup\n",
    "\n",
    "df = df_scl.copy()\n",
    "features = [col for col in df.columns if col.startswith(\"x_\")]\n",
    "clients = df[\"client_id\"].unique()\n",
    "months = sorted(df[\"label\"].unique())\n",
    "target_client = \"District_E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d243e2-6480-4566-9fd4-73029b8be85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for month in tqdm(months):\n",
    "    df_month = df[df.label == month]\n",
    "    X_target = df_month[df_month.client_id == target_client][features].values\n",
    "\n",
    "    try:\n",
    "        spectral_target = spectral_cluster_latent(df_month[df_month.client_id == target_client])\n",
    "    except Exception:\n",
    "        spectral_target = None  # fallback\n",
    "\n",
    "    for other_client in clients:\n",
    "        if other_client == target_client:\n",
    "            continue\n",
    "\n",
    "        X_other = df_month[df_month.client_id == other_client][features].values\n",
    "        if X_target.size == 0 or X_other.size == 0:\n",
    "            continue\n",
    "\n",
    "        # 1. Distribution metrics\n",
    "        mmd_val, w_dist, e_dist = compare_clients_distribution(X_target, X_other)\n",
    "\n",
    "        # 2. Subspace alignment\n",
    "        sa = subspace_alignment(X_target, X_other)\n",
    "\n",
    "        # 3. DTW over historical latent trajectory\n",
    "        traj_target = dtw_client_trajectory(df[df.label <= month], target_client)\n",
    "        traj_other = dtw_client_trajectory(df[df.label <= month], other_client)\n",
    "        min_len = min(len(traj_target), len(traj_other))\n",
    "        dtw_val, _ = fastdtw(traj_target[:min_len], traj_other[:min_len], dist=euclidean)\n",
    "\n",
    "        # 4. Mutual Information\n",
    "        mi = compute_mi(df, target_client, other_client, month)\n",
    "\n",
    "        # 5. KL & JSD\n",
    "        kl = compute_kl_divergence(X_target, X_other)\n",
    "        jsd = compute_js_divergence(X_target, X_other)\n",
    "        results.append({\n",
    "            \"month\": month,\n",
    "            \"target_client\": target_client,\n",
    "            \"other_client\": other_client,\n",
    "            \"MMD\": mmd_val,\n",
    "            \"Wasserstein\": w_dist,\n",
    "            \"Energy\": e_dist,\n",
    "            \"SubspaceAlignment\": sa,\n",
    "            \"DTW\": dtw_val,\n",
    "            \"MutualInfo\": mi,\n",
    "            \"KL\": kl,\n",
    "            \"JSD\": jsd,\n",
    "            \"TargetNumClusters\": len(set(labels_target)) - (1 if -1 in labels_target else 0),\n",
    "        })\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973def4-e727-4f6b-8047-92d44e35e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"MMD\", \"Wasserstein\", \"Energy\", \"SubspaceAlignment\", \"DTW\", \"MutualInfo\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_results, x=\"month\", y=metric, hue=\"other_client\", marker=\"o\")\n",
    "    plt.title(f\"{metric} Drift vs {target_client}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3537e-3323-4276-a303-e0d29473c024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af92dd-2fb0-49ef-91c5-898644162501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe23eee-8cc6-4564-a9ba-c3caad0888ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "client_monthly_embeddings = []\n",
    "spectral_embeddings = []\n",
    "\n",
    "umap_model = UMAP(n_components=2, random_state=42)\n",
    "pca_model = PCA(n_components=2)\n",
    "\n",
    "results = []\n",
    "\n",
    "for month in tqdm(months):\n",
    "    df_month = df[df.label == month]\n",
    "    X_target = df_month[df_month.client_id == target_client][features].values\n",
    "    labels_target = run_dbscan(df_month[df_month.client_id == target_client])\n",
    "    try:\n",
    "        spectral_target = spectral_cluster_latent(df_month[df_month.client_id == target_client])\n",
    "    except Exception:\n",
    "        spectral_target = None  # fallback\n",
    "\n",
    "    for other_client in clients:\n",
    "        for client in clients:\n",
    "            client_data = df_month[df_month.client_id == client][features]\n",
    "            if client_data.empty:\n",
    "                continue\n",
    "    \n",
    "            # Monthly mean latent embedding for PCA/UMAP trajectory\n",
    "            mean_emb = client_data.mean().values\n",
    "            client_monthly_embeddings.append({\n",
    "                \"client_id\": client,\n",
    "                \"month\": month,\n",
    "                \"x\": mean_emb\n",
    "            })\n",
    "    \n",
    "            # Save spectral embedding\n",
    "            try:\n",
    "                spectral = spectral_cluster_latent(client_data)\n",
    "                for i, emb in enumerate(spectral):\n",
    "                    spectral_embeddings.append({\n",
    "                        \"client_id\": client,\n",
    "                        \"month\": month,\n",
    "                        \"point_id\": i,\n",
    "                        \"spectral_x\": emb[0],\n",
    "                        \"spectral_y\": emb[1]\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                continue  # skip malformed inputs\n",
    "                \n",
    "        if other_client == target_client:\n",
    "            continue\n",
    "\n",
    "        X_other = df_month[df_month.client_id == other_client][features].values\n",
    "        if X_target.size == 0 or X_other.size == 0:\n",
    "            continue\n",
    "\n",
    "        # 1. Distribution metrics\n",
    "        mmd_val, w_dist, e_dist = compare_clients_distribution(X_target, X_other)\n",
    "\n",
    "        # 2. Subspace alignment\n",
    "        sa = subspace_alignment(X_target, X_other)\n",
    "\n",
    "        # 3. DTW over historical latent trajectory\n",
    "        traj_target = dtw_client_trajectory(df[df.label <= month], target_client)\n",
    "        traj_other = dtw_client_trajectory(df[df.label <= month], other_client)\n",
    "        min_len = min(len(traj_target), len(traj_other))\n",
    "        dtw_val, _ = fastdtw(traj_target[:min_len], traj_other[:min_len], dist=euclidean)\n",
    "\n",
    "        # 4. Mutual Information\n",
    "        mi = compute_mi(df, target_client, other_client, month)\n",
    "\n",
    "        results.append({\n",
    "            \"month\": month,\n",
    "            \"target_client\": target_client,\n",
    "            \"other_client\": other_client,\n",
    "            \"MMD\": mmd_val,\n",
    "            \"Wasserstein\": w_dist,\n",
    "            \"Energy\": e_dist,\n",
    "            \"SubspaceAlignment\": sa,\n",
    "            \"DTW\": dtw_val,\n",
    "            \"MutualInfo\": mi,\n",
    "        })\n",
    "\n",
    "df_traj = pd.DataFrame(client_monthly_embeddings)\n",
    "df_spectral = pd.DataFrame(spectral_embeddings)\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dd972-7eba-4269-a918-c7517e3225c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"MMD\", \"Wasserstein\", \"Energy\", \"SubspaceAlignment\", \"DTW\", \"MutualInfo\"]\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_results, x=\"month\", y=metric, hue=\"other_client\", marker=\"o\")\n",
    "    plt.title(f\"{metric} Drift vs {target_client}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a4c57-4a57-4573-8ef2-d9a1ba3a5a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand latent vector\n",
    "df_traj = pd.DataFrame(client_monthly_embeddings)\n",
    "df_traj_features = pd.DataFrame(df_traj[\"x\"].tolist())\n",
    "df_traj = pd.concat([df_traj.drop(columns=\"x\"), df_traj_features], axis=1)\n",
    "df_traj.columns = ['client_id', 'month'] + features\n",
    "\n",
    "X_latent = df_traj[features].values\n",
    "df_traj[[\"pca_x\", \"pca_y\"]] = pca_model.fit_transform(X_latent)\n",
    "df_traj[[\"umap_x\", \"umap_y\"]] = umap_model.fit_transform(X_latent)\n",
    "\n",
    "\n",
    "# PCA Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_traj, x=\"pca_x\", y=\"pca_y\", hue=\"client_id\", marker=\"o\", palette=\"tab10\")\n",
    "plt.title(\"Client Latent Trajectories - PCA\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# UMAP Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_traj, x=\"umap_x\", y=\"umap_y\", hue=\"client_id\", marker=\"o\", palette=\"tab10\")\n",
    "plt.title(\"Client Latent Trajectories - UMAP\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9032fec-d809-4fa8-ac34-3ca28acc99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.drift import ADWIN\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clients = df_traj[\"client_id\"].unique()\n",
    "months = sorted(df_traj[\"month\"].unique())\n",
    "\n",
    "# Initialize drift matrix\n",
    "drift_matrix = pd.DataFrame(index=clients, columns=months)\n",
    "\n",
    "for client in clients:\n",
    "    detector = ADWIN()\n",
    "    client_data = df_traj[df_traj[\"client_id\"] == client].sort_values(\"month\")\n",
    "\n",
    "    for _, row in client_data.iterrows():\n",
    "        val = row[\"pca_x\"]  # or pca_y, umap_x, etc.\n",
    "        detector.update(val)\n",
    "        drift = int(detector.drift_detected)  # âœ… River's correct attribute\n",
    "        drift_matrix.loc[client, row[\"month\"]] = drift\n",
    "\n",
    "drift_matrix.fillna(0, inplace=True)\n",
    "drift_matrix = drift_matrix.astype(int)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(drift_matrix, cmap=\"Reds\", cbar_kws={\"label\": \"Drift Detected\"}, linewidths=0.5, linecolor='gray', square=True)\n",
    "plt.title(\"River ADWIN Drift Detection Heatmap (PCA-X)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Client ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47978c6e-e9a8-429c-b65b-ac097003375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.drift import ADWIN, EDDM, PageHinkley, HDDM_W\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_drift_detector(name):\n",
    "    \"\"\"Factory function for River drift detectors.\"\"\"\n",
    "    detectors = {\n",
    "        \"adwin\": ADWIN,\n",
    "        \"eddm\": EDDM,\n",
    "        \"pagehinkley\": PageHinkley,\n",
    "        \"hddm\": HDDM_W\n",
    "    }\n",
    "    return detectors[name.lower()]()\n",
    "\n",
    "def detect_client_drift(\n",
    "    df,\n",
    "    target_client,\n",
    "    feature_cols,\n",
    "    detector_type=\"adwin\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects drift month-by-month for a target client compared to all others,\n",
    "    using a specified drift detector and multiple dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with columns ['client_id', 'month', ...feature_cols]\n",
    "    - target_client: ID of the target client\n",
    "    - feature_cols: List of columns to use (latent/PCA/UMAP)\n",
    "    - detector_type: Drift detector ('adwin', 'eddm', 'pagehinkley', 'hddm')\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series with drift signal per month\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    months = sorted(df[\"month\"].unique())\n",
    "    drift_series = []\n",
    "\n",
    "    detector = get_drift_detector(detector_type)\n",
    "\n",
    "    for month in months:\n",
    "        df_month = df[df[\"month\"] == month]\n",
    "        target_rows = df_month[df_month[\"client_id\"] == target_client]\n",
    "        other_rows = df_month[df_month[\"client_id\"] != target_client]\n",
    "\n",
    "        # Mean over multiple samples for stability\n",
    "        target_mean = target_rows[feature_cols].mean().values\n",
    "\n",
    "        # Compare with all other clients (average difference)\n",
    "        drift_input = 0\n",
    "        count = 0\n",
    "        for _, row in other_rows.iterrows():\n",
    "            other_vector = row[feature_cols].values\n",
    "            # Compute vector difference magnitude\n",
    "            diff = np.linalg.norm(target_mean - other_vector)\n",
    "            detector.update(diff)\n",
    "            count += 1\n",
    "\n",
    "        drift = int(detector.drift_detected) if count > 0 else 0\n",
    "        drift_series.append((month, drift))\n",
    "\n",
    "    return pd.Series({m: d for m, d in drift_series}, name=target_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f9eda-dd39-458c-b9d7-81304701f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = client_data.copy()\n",
    "\n",
    "client_id = \"District_E\"\n",
    "feature_columns = [\"pca_x\", \"pca_y\"]  # or x_0 to x_39\n",
    "\n",
    "drift = detect_client_drift(\n",
    "    df=df_pca,\n",
    "    target_client=client_id,\n",
    "    feature_cols=feature_columns,\n",
    "    detector_type=\"eddm\"\n",
    ")\n",
    "\n",
    "all_clients = df_pca[\"client_id\"].unique()\n",
    "drift_all = pd.DataFrame()\n",
    "\n",
    "for client in all_clients:\n",
    "    drift_all[client] = detect_client_drift(\n",
    "        df=df_pca,\n",
    "        target_client=client,\n",
    "        feature_cols=feature_columns,\n",
    "        detector_type=\"adwin\"  # can loop over detector types too\n",
    "    )\n",
    "\n",
    "# Transpose for heatmap\n",
    "drift_all = drift_all.T.fillna(0).astype(int)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(drift_all, cmap=\"Reds\", cbar_kws={\"label\": \"Drift Detected\"}, linewidths=0.5)\n",
    "plt.title(\"Client Drift Heatmap\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Client ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208d43a-9217-4fbd-b0f9-655acb663da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from river.drift import ADWIN, EDDM, PageHinkley, HDDM_W\n",
    "\n",
    "def get_drift_detector(name):\n",
    "    detectors = {\n",
    "        \"adwin\": ADWIN,\n",
    "        \"eddm\": EDDM,\n",
    "        \"pagehinkley\": PageHinkley,\n",
    "        \"hddm\": HDDM_W\n",
    "    }\n",
    "    return detectors[name.lower()]()\n",
    "\n",
    "def compute_drift_intensity(\n",
    "    df,\n",
    "    target_client,\n",
    "    feature_cols,\n",
    "    detector_type=\"adwin\",\n",
    "    normalize=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes drift intensity (mean distance from others) and optionally drift detection signals.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    months = sorted(df[\"month\"].unique())\n",
    "    results = []\n",
    "    detector = get_drift_detector(detector_type) if detector_type else None\n",
    "\n",
    "    for month in months:\n",
    "        df_month = df[df[\"month\"] == month]\n",
    "        target_rows = df_month[df_month[\"client_id\"] == target_client]\n",
    "        other_rows = df_month[df_month[\"client_id\"] != target_client]\n",
    "\n",
    "        if len(target_rows) == 0 or len(other_rows) == 0:\n",
    "            # Skipping this month due to missing data\n",
    "            continue\n",
    "\n",
    "        target_mean = target_rows[feature_cols].mean().values\n",
    "\n",
    "        distances = []\n",
    "        for _, row in other_rows.iterrows():\n",
    "            other_vector = row[feature_cols].values\n",
    "            dist = np.linalg.norm(target_mean - other_vector)\n",
    "            distances.append(dist)\n",
    "            if detector:\n",
    "                detector.update(dist)\n",
    "\n",
    "        intensity = np.mean(distances)\n",
    "        drift_flag = int(detector.drift_detected) if detector else None\n",
    "\n",
    "        results.append({\n",
    "            \"month\": month,\n",
    "            \"intensity\": intensity,\n",
    "            \"drift\": drift_flag\n",
    "        })\n",
    "\n",
    "    if not results:\n",
    "        print(f\"[WARN] No data available to compute drift for client '{target_client}'.\")\n",
    "        return pd.DataFrame(columns=[\"month\", \"intensity\", \"drift\"])\n",
    "\n",
    "    df_result = pd.DataFrame(results)\n",
    "    df_result.set_index(\"month\", inplace=True)\n",
    "\n",
    "    if normalize and not df_result[\"intensity\"].isnull().all():\n",
    "        df_result[\"intensity\"] = (\n",
    "            (df_result[\"intensity\"] - df_result[\"intensity\"].min()) /\n",
    "            (df_result[\"intensity\"].max() - df_result[\"intensity\"].min() + 1e-8)\n",
    "        )\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3eca62-ee1b-4566-b650-aaa93e22ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = \"District_E\"\n",
    "features = [f\"x_{i}\" for i in range(40)]  # or PCA/UMAP components\n",
    "df_pca = client_data.copy()\n",
    "\n",
    "intensity_df = compute_drift_intensity(\n",
    "    df=df_pca,  # your full latent/PCA DataFrame\n",
    "    target_client=client,\n",
    "    feature_cols=features,\n",
    "    detector_type=\"pagehinkley\",  # optional\n",
    "    normalize=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a2879-e08f-4d75-9f7e-d83f86963784",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a4388-d807-4fee-aac7-65fa7ba79cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6fad8-81f4-4f44-bc1e-13612a2a3395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2957acc0-01ce-44bf-a053-7654102ffe36",
   "metadata": {},
   "source": [
    "prompt:\n",
    "Im working with representations of temporal series - which are basically the feature extraction on the latent space of an autoencoder. The context is to detect drift in the federated learning context, in a macrocontext of leakage detection - so the data is time series.\n",
    "\n",
    "So, instead of using data, im aiming to use these latent representations to assert similarities or changes between clients. I'm using contrastive learning to make m model put diferent months observations in diferent regions on the latent space.\n",
    "\n",
    "I dont want to use only distances to measure and compare changes/similarities between clients, can ou please suggest some strategies to assert that?\n",
    "\n",
    "Response:\n",
    "ðŸ”„ 1. Distributional Similarity Metrics (Beyond Distance)\n",
    "Instead of Euclidean or cosine distances, measure how entire distributions in latent space shift over time:\n",
    "\n",
    "Maximum Mean Discrepancy (MMD): A kernel-based method to compare two distributions in latent space. Useful to detect subtle shifts.\n",
    "\n",
    "Wasserstein Distance (Earth Moverâ€™s Distance): Measures the effort to transform one distribution into another. Good for temporal shift.\n",
    "\n",
    "Energy Distance: Another distribution-based metric that captures divergence in samples.\n",
    "\n",
    "Kernel Two-Sample Test (e.g., HSIC): To statistically test if two samples (e.g., client A's latent representation from month 1 and month 2) come from the same distribution.\n",
    "\n",
    "ðŸ§  2. Representation Clustering and Topology\n",
    "Capture structural or geometric patterns in the latent space:\n",
    "\n",
    "Persistent Homology / Topological Data Analysis (TDA): Use persistence diagrams to measure topological changes in the manifold structure of embeddings.\n",
    "\n",
    "Spectral Clustering on Latent Graphs: Create similarity graphs of representations and track changes in graph eigenvectors (Laplacian spectrum) over time.\n",
    "\n",
    "Density-based Clustering (e.g., DBSCAN) and Cluster Drift: Track number and shape of clusters in latent space across months/clients.\n",
    "\n",
    "ðŸ§¬ 3. Manifold/Embedding Alignment Techniques\n",
    "Compare latent spaces geometrically over time, without relying on direct distances:\n",
    "\n",
    "Procrustes Analysis: Align embeddings from two time steps and measure the transformation required.\n",
    "\n",
    "Canonical Correlation Analysis (CCA): Measure the linear relationship between two sets of latent variables (month t and t+1).\n",
    "\n",
    "Geodesic Flow Kernel / Subspace Alignment: Used in domain adaptation; track how subspaces evolve between clients/timesteps.\n",
    "\n",
    "ðŸ§ª 4. Contrastive Representation Drift Probes\n",
    "Given you're using contrastive learning, you can extend that idea to probe the drift directly:\n",
    "\n",
    "Train a contrastive classifier to distinguish latent representations from two months. If it performs well, they are separable (drift has occurred).\n",
    "\n",
    "Hard negative mining: If previously similar clients start appearing as hard negatives, it can indicate drift.\n",
    "\n",
    "Class separability metrics (e.g., Fisher criterion, SVM margin width) in latent space across time.\n",
    "\n",
    "ðŸ” 5. Mutual Information and Entropy Measures\n",
    "Capture shared information or uncertainty:\n",
    "\n",
    "Mutual Information (MI) between representations of different months/clients.\n",
    "\n",
    "Conditional Entropy: How predictable is month t+1 given month t representations?\n",
    "\n",
    "Information Bottleneck (IB): How much information about the client/time label is retained in latent space?\n",
    "\n",
    "ðŸ§­ 6. Temporal Dynamics Modeling\n",
    "Model the latent representation trajectory over time:\n",
    "\n",
    "Trajectory Embedding Comparison: Encode client evolution over time as sequences and compare them via DTW (Dynamic Time Warping), or RNN embeddings.\n",
    "\n",
    "Autoregressive Models: Fit an AR or RNN model to latent vectors. Changes in residuals or coefficients can signal drift.\n",
    "\n",
    "Latent ODEs: Capture continuous-time dynamics of latent representations.\n",
    "\n",
    "ðŸ“Š 7. Statistical Testing and Outlier Detection\n",
    "Change Point Detection Algorithms (e.g., RuLSIF, Bayesian Online Change Point Detection) applied on latent representations per client.\n",
    "\n",
    "Anomaly scores (Isolation Forests, One-Class SVMs) trained on earlier latent space periods and applied to newer ones.\n",
    "\n",
    "Permutation-based drift tests, e.g., apply permutation importance in a classifier trained to discriminate client/time labels.\n",
    "\n",
    "Integration Strategy (Macrocontext-aware)\n",
    "Since you're in a federated learning + leakage detection context:\n",
    "\n",
    "Use cross-client metrics to capture common or divergent patterns (e.g., align or cluster client trajectories).\n",
    "\n",
    "Consider federated aggregation of latent drift scores for global drift awareness without centralizing raw data.\n",
    "\n",
    "Monitor temporal consistency of latent features within and across clients, possibly using privacy-preserving sketching methods.\n",
    "\n",
    "\n",
    "Data:\n",
    "Labels represent months, client_id the clients and the remaining are features of the latent space, such as\n",
    "\n",
    "\tlabel\tclient_id\tx_0\tx_1\tx_2\tx_3\tx_4\tx_5\tx_6\tx_7\t...\tx_30\tx_31\tx_32\tx_33\tx_34\tx_35\tx_36\tx_37\tx_38\tx_39\n",
    "0\t1\tDistrict_A\t0.346425\t0.528547\t0.345281\t0.252769\t0.271071\t0.525118\t0.952367\t0.338785\t...\t0.885777\t0.921251\t0.812938\t0.327208\t0.960554\t0.727612\t0.189739\t0.134118\t0.666391\t0.730167\n",
    "1\t1\tDistrict_A\t0.922042\t0.651472\t0.149660\t0.476432\t0.346260\t0.640628\t0.950953\t0.714316\t...\t0.835812\t0.899418\t0.731884\t0.419002\t0.954092\t0.717899\t0.116916\t0.096327\t0.666886\t0.819726\n",
    "\n",
    "Can you please help me check the diffs/similarities between clients over the months (labels), using:\n",
    "\n",
    "1. Maximum Mean Discrepancy (MMD) and Wasserstein Distance and Energy Distance\n",
    "\n",
    "2. Spectral Clustering on Latent Graphs and Density-based Clustering (e.g., DBSCAN)\n",
    "\n",
    "3. Geodesic Flow Kernel / Subspace Alignment\n",
    "\n",
    "4. Mutual Information (MI), Conditional Entropy and Information Bottleneck (IB)\n",
    "\n",
    "5. Trajectory Embedding Comparison: Encode client evolution over time as sequences and compare them via DTW (Dynamic Time Warping)\n",
    "\n",
    "Can you please help me with that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2958266-b160-477f-a211-90506580701c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TsLeaks]",
   "language": "python",
   "name": "conda-env-TsLeaks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
